{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 6: Synaptic Plasticity\n",
    "\n",
    "**Duration:** ~40 minutes | **Prerequisites:** Basic Tutorials, Tutorial 5\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- âœ… Understand short-term plasticity (STP) mechanisms\n",
    "- âœ… Implement synaptic depression and facilitation\n",
    "- âœ… Learn spike-timing-dependent plasticity (STDP) principles\n",
    "- âœ… Create adaptive synapses with learning rules\n",
    "- âœ… Build networks with plastic connections\n",
    "- âœ… Combine plasticity with network training\n",
    "\n",
    "## Overview\n",
    "\n",
    "Synaptic plasticity is the ability of synapses to change their strength over time. This is fundamental to learning and memory in biological brains. BrainPy supports multiple forms of plasticity:\n",
    "\n",
    "**Types of plasticity:**\n",
    "- **Short-term plasticity (STP)**: Temporary changes on timescales of milliseconds to seconds\n",
    "  - Depression (STD): Synaptic strength decreases with repeated use\n",
    "  - Facilitation (STF): Synaptic strength increases with repeated use\n",
    "- **Long-term plasticity**: Persistent changes\n",
    "  - STDP: Depends on relative timing of pre and post spikes\n",
    "  - Rate-based: Depends on firing rates\n",
    "\n",
    "Let's explore these mechanisms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainstate\n",
    "import brainunit as u\n",
    "import braintools\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "brainstate.random.seed(42)\n",
    "\n",
    "# Configure environment\n",
    "brainstate.environ.set(dt=0.1 * u.ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Short-Term Depression (STD)\n",
    "\n",
    "Short-term depression models the depletion of neurotransmitter resources. Each spike consumes some fraction of available resources, which recover over time.\n",
    "\n",
    "**STD dynamics:**\n",
    "$$\n",
    "\\frac{dx}{dt} = \\frac{1 - x}{\\tau_d} - u \\cdot x \\cdot \\delta(t - t_{spike})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x$: Fraction of available resources (0 to 1)\n",
    "- $\\tau_d$: Recovery time constant\n",
    "- $u$: Utilization fraction per spike\n",
    "\n",
    "**Effect:** Repeated spikes deplete resources â†’ synaptic current decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synapse with short-term depression\n",
    "class STDSynapse(bp.Synapse):\n",
    "    \"\"\"Synapse with short-term depression.\"\"\"\n",
    "    \n",
    "    def __init__(self, size, tau=5.0*u.ms, tau_d=200.0*u.ms, U=0.5, **kwargs):\n",
    "        super().__init__(size, **kwargs)\n",
    "        \n",
    "        # Synapse parameters\n",
    "        self.tau = tau  # Synaptic time constant\n",
    "        self.tau_d = tau_d  # Depression time constant\n",
    "        self.U = U  # Utilization fraction\n",
    "        \n",
    "        # States\n",
    "        self.g = brainstate.ShortTermState(jnp.zeros(size))  # Conductance\n",
    "        self.x = brainstate.ShortTermState(jnp.ones(size))  # Available resources\n",
    "    \n",
    "    def reset_state(self, batch_size=None):\n",
    "        self.g.value = jnp.zeros(self.size if batch_size is None else (batch_size, self.size))\n",
    "        self.x.value = jnp.ones(self.size if batch_size is None else (batch_size, self.size))\n",
    "    \n",
    "    def update(self, pre_spike):\n",
    "        # Get time step\n",
    "        dt = brainstate.environ.get_dt()\n",
    "        \n",
    "        # Depression: reduce available resources on spike\n",
    "        x_new = self.x.value + pre_spike * (-self.U * self.x.value)\n",
    "        \n",
    "        # Recovery: exponential recovery of resources\n",
    "        dx = (1.0 - x_new) / self.tau_d.to_decimal(u.ms) * dt.to_decimal(u.ms)\n",
    "        self.x.value = x_new + dx\n",
    "        \n",
    "        # Synaptic current: modulated by available resources\n",
    "        dg = -self.g.value / self.tau.to_decimal(u.ms) * dt.to_decimal(u.ms)\n",
    "        self.g.value += dg + pre_spike * self.U * self.x.value\n",
    "        \n",
    "        return self.g.value\n",
    "\n",
    "# Test with spike train\n",
    "std_syn = STDSynapse(size=1, tau=5.0*u.ms, tau_d=200.0*u.ms, U=0.5)\n",
    "brainstate.nn.init_all_states(std_syn)\n",
    "\n",
    "# Generate spike train: 10 spikes at 20 Hz\n",
    "duration = 1000 * u.ms\n",
    "n_steps = int(duration / brainstate.environ.get_dt())\n",
    "spike_times = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]  # in ms\n",
    "spike_indices = [int(t / 0.1) for t in spike_times]\n",
    "\n",
    "# Simulate\n",
    "g_history = []\n",
    "x_history = []\n",
    "\n",
    "for i in range(n_steps):\n",
    "    spike = 1.0 if i in spike_indices else 0.0\n",
    "    g = std_syn(spike)\n",
    "    g_history.append(float(g))\n",
    "    x_history.append(float(std_syn.x.value))\n",
    "\n",
    "# Plot\n",
    "times = np.arange(n_steps) * 0.1\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Synaptic conductance\n",
    "axes[0].plot(times, g_history, 'b-', linewidth=2)\n",
    "for st in spike_times:\n",
    "    axes[0].axvline(st, color='r', linestyle='--', alpha=0.3)\n",
    "axes[0].set_ylabel('Synaptic Conductance g', fontsize=12)\n",
    "axes[0].set_title('Short-Term Depression', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Available resources\n",
    "axes[1].plot(times, x_history, 'g-', linewidth=2)\n",
    "for st in spike_times:\n",
    "    axes[1].axvline(st, color='r', linestyle='--', alpha=0.3, label='Spike' if st == spike_times[0] else '')\n",
    "axes[1].set_xlabel('Time (ms)', fontsize=12)\n",
    "axes[1].set_ylabel('Available Resources x', fontsize=12)\n",
    "axes[1].set_title('Resource Depletion and Recovery', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š STD Observations:\")\n",
    "print(\"   â€¢ Each spike depletes available resources\")\n",
    "print(\"   â€¢ Synaptic conductance decreases with repeated spikes\")\n",
    "print(\"   â€¢ Resources recover exponentially between spikes\")\n",
    "print(\"   â€¢ Implements synaptic fatigue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Short-Term Facilitation (STF)\n",
    "\n",
    "Short-term facilitation models the buildup of calcium in the presynaptic terminal, which increases neurotransmitter release probability.\n",
    "\n",
    "**STF dynamics:**\n",
    "$$\n",
    "\\frac{du}{dt} = \\frac{U - u}{\\tau_f} + U(1 - u) \\cdot \\delta(t - t_{spike})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $u$: Utilization parameter (increases with spikes)\n",
    "- $\\tau_f$: Facilitation time constant\n",
    "- $U$: Baseline utilization\n",
    "\n",
    "**Effect:** Repeated spikes increase utilization â†’ synaptic current increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STFSynapse(bp.Synapse):\n",
    "    \"\"\"Synapse with short-term facilitation.\"\"\"\n",
    "    \n",
    "    def __init__(self, size, tau=5.0*u.ms, tau_f=200.0*u.ms, U=0.15, **kwargs):\n",
    "        super().__init__(size, **kwargs)\n",
    "        \n",
    "        self.tau = tau\n",
    "        self.tau_f = tau_f  # Facilitation time constant\n",
    "        self.U = U  # Baseline utilization\n",
    "        \n",
    "        # States\n",
    "        self.g = brainstate.ShortTermState(jnp.zeros(size))\n",
    "        self.u = brainstate.ShortTermState(jnp.ones(size) * U)  # Current utilization\n",
    "    \n",
    "    def reset_state(self, batch_size=None):\n",
    "        self.g.value = jnp.zeros(self.size if batch_size is None else (batch_size, self.size))\n",
    "        self.u.value = jnp.ones(self.size if batch_size is None else (batch_size, self.size)) * self.U\n",
    "    \n",
    "    def update(self, pre_spike):\n",
    "        dt = brainstate.environ.get_dt()\n",
    "        \n",
    "        # Facilitation: increase utilization on spike\n",
    "        u_new = self.u.value + pre_spike * (self.U * (1.0 - self.u.value))\n",
    "        \n",
    "        # Decay: exponential decay of facilitation\n",
    "        du = -(u_new - self.U) / self.tau_f.to_decimal(u.ms) * dt.to_decimal(u.ms)\n",
    "        self.u.value = u_new + du\n",
    "        \n",
    "        # Synaptic current: modulated by current utilization\n",
    "        dg = -self.g.value / self.tau.to_decimal(u.ms) * dt.to_decimal(u.ms)\n",
    "        self.g.value += dg + pre_spike * self.u.value\n",
    "        \n",
    "        return self.g.value\n",
    "\n",
    "# Test facilitation\n",
    "stf_syn = STFSynapse(size=1, tau=5.0*u.ms, tau_f=200.0*u.ms, U=0.15)\n",
    "brainstate.nn.init_all_states(stf_syn)\n",
    "\n",
    "# Same spike train as STD\n",
    "g_history_f = []\n",
    "u_history = []\n",
    "\n",
    "for i in range(n_steps):\n",
    "    spike = 1.0 if i in spike_indices else 0.0\n",
    "    g = stf_syn(spike)\n",
    "    g_history_f.append(float(g))\n",
    "    u_history.append(float(stf_syn.u.value))\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# STD conductance\n",
    "axes[0].plot(times, g_history, 'b-', linewidth=2, label='STD')\n",
    "for st in spike_times:\n",
    "    axes[0].axvline(st, color='r', linestyle='--', alpha=0.2)\n",
    "axes[0].set_ylabel('Conductance', fontsize=12)\n",
    "axes[0].set_title('Depression: Decreasing Response', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# STF conductance\n",
    "axes[1].plot(times, g_history_f, 'g-', linewidth=2, label='STF')\n",
    "for st in spike_times:\n",
    "    axes[1].axvline(st, color='r', linestyle='--', alpha=0.2)\n",
    "axes[1].set_ylabel('Conductance', fontsize=12)\n",
    "axes[1].set_title('Facilitation: Increasing Response', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Utilization parameter\n",
    "axes[2].plot(times, u_history, 'm-', linewidth=2)\n",
    "for st in spike_times:\n",
    "    axes[2].axvline(st, color='r', linestyle='--', alpha=0.2, label='Spike' if st == spike_times[0] else '')\n",
    "axes[2].set_xlabel('Time (ms)', fontsize=12)\n",
    "axes[2].set_ylabel('Utilization u', fontsize=12)\n",
    "axes[2].set_title('Facilitation Buildup', fontsize=14, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š STF vs STD:\")\n",
    "print(\"   STD: Synaptic strength DECREASES with repeated use\")\n",
    "print(\"   STF: Synaptic strength INCREASES with repeated use\")\n",
    "print(\"   Both effects are temporary (100s of ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Combined STP (Depression + Facilitation)\n",
    "\n",
    "Real synapses often exhibit both depression and facilitation. BrainPy provides a combined STP model.\n",
    "\n",
    "**Combined dynamics:**\n",
    "- Depression: Resource depletion with time constant $\\tau_d$\n",
    "- Facilitation: Utilization increase with time constant $\\tau_f$\n",
    "- Effective synaptic current: $g_{eff} = u \\cdot x \\cdot g$\n",
    "\n",
    "Depending on relative values of $\\tau_d$, $\\tau_f$, and $U$, synapses can be:\n",
    "- **Depressing**: $\\tau_d \\gg \\tau_f$, large $U$\n",
    "- **Facilitating**: $\\tau_f \\gg \\tau_d$, small $U$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BrainPy's built-in STP model\n",
    "# For demonstration, we'll test different parameter regimes\n",
    "\n",
    "def simulate_stp(tau_f, tau_d, U, spike_indices, n_steps, label):\n",
    "    \"\"\"Simulate STP synapse and return conductance history.\"\"\"\n",
    "    \n",
    "    class STPSynapse(bp.Synapse):\n",
    "        def __init__(self, size, **kwargs):\n",
    "            super().__init__(size, **kwargs)\n",
    "            self.tau = 5.0 * u.ms\n",
    "            self.tau_f = tau_f\n",
    "            self.tau_d = tau_d\n",
    "            self.U = U\n",
    "            self.g = brainstate.ShortTermState(jnp.zeros(size))\n",
    "            self.x = brainstate.ShortTermState(jnp.ones(size))\n",
    "            self.u = brainstate.ShortTermState(jnp.ones(size) * U)\n",
    "        \n",
    "        def reset_state(self, batch_size=None):\n",
    "            self.g.value = jnp.zeros(self.size if batch_size is None else (batch_size, self.size))\n",
    "            self.x.value = jnp.ones(self.size if batch_size is None else (batch_size, self.size))\n",
    "            self.u.value = jnp.ones(self.size if batch_size is None else (batch_size, self.size)) * self.U\n",
    "        \n",
    "        def update(self, pre_spike):\n",
    "            dt = brainstate.environ.get_dt()\n",
    "            \n",
    "            # Facilitation\n",
    "            u_new = self.u.value + pre_spike * (self.U * (1.0 - self.u.value))\n",
    "            du = -(u_new - self.U) / self.tau_f.to_decimal(u.ms) * dt.to_decimal(u.ms)\n",
    "            self.u.value = u_new + du\n",
    "            \n",
    "            # Depression\n",
    "            x_new = self.x.value + pre_spike * (-self.u.value * self.x.value)\n",
    "            dx = (1.0 - x_new) / self.tau_d.to_decimal(u.ms) * dt.to_decimal(u.ms)\n",
    "            self.x.value = x_new + dx\n",
    "            \n",
    "            # Conductance\n",
    "            dg = -self.g.value / self.tau.to_decimal(u.ms) * dt.to_decimal(u.ms)\n",
    "            self.g.value += dg + pre_spike * self.u.value * self.x.value\n",
    "            \n",
    "            return self.g.value\n",
    "    \n",
    "    syn = STPSynapse(size=1)\n",
    "    brainstate.nn.init_all_states(syn)\n",
    "    \n",
    "    g_hist = []\n",
    "    for i in range(n_steps):\n",
    "        spike = 1.0 if i in spike_indices else 0.0\n",
    "        g = syn(spike)\n",
    "        g_hist.append(float(g))\n",
    "    \n",
    "    return g_hist\n",
    "\n",
    "# Three parameter regimes\n",
    "g_depressing = simulate_stp(\n",
    "    tau_f=50.0*u.ms, tau_d=400.0*u.ms, U=0.6,\n",
    "    spike_indices=spike_indices, n_steps=n_steps, label='Depressing'\n",
    ")\n",
    "\n",
    "g_facilitating = simulate_stp(\n",
    "    tau_f=400.0*u.ms, tau_d=50.0*u.ms, U=0.1,\n",
    "    spike_indices=spike_indices, n_steps=n_steps, label='Facilitating'\n",
    ")\n",
    "\n",
    "g_mixed = simulate_stp(\n",
    "    tau_f=200.0*u.ms, tau_d=200.0*u.ms, U=0.3,\n",
    "    spike_indices=spike_indices, n_steps=n_steps, label='Mixed'\n",
    ")\n",
    "\n",
    "# Plot all three\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(times, g_depressing, 'b-', linewidth=2, label='Depressing (large U, slow recovery)', alpha=0.8)\n",
    "ax.plot(times, g_facilitating, 'g-', linewidth=2, label='Facilitating (small U, fast recovery)', alpha=0.8)\n",
    "ax.plot(times, g_mixed, 'm-', linewidth=2, label='Mixed (balanced)', alpha=0.8)\n",
    "\n",
    "for st in spike_times:\n",
    "    ax.axvline(st, color='r', linestyle='--', alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('Time (ms)', fontsize=12)\n",
    "ax.set_ylabel('Synaptic Conductance', fontsize=12)\n",
    "ax.set_title('Short-Term Plasticity: Parameter Regimes', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š STP Parameter Regimes:\")\n",
    "print(\"   Blue (Depressing): High U, slow depression recovery\")\n",
    "print(\"   Green (Facilitating): Low U, fast depression recovery\")\n",
    "print(\"   Magenta (Mixed): Balanced parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Spike-Timing-Dependent Plasticity (STDP)\n",
    "\n",
    "STDP is a form of long-term plasticity where synaptic strength changes depend on the relative timing of pre- and postsynaptic spikes.\n",
    "\n",
    "**STDP rule:**\n",
    "- **Potentiation**: If pre-spike occurs before post-spike ($\\Delta t > 0$), strengthen synapse\n",
    "- **Depression**: If post-spike occurs before pre-spike ($\\Delta t < 0$), weaken synapse\n",
    "\n",
    "**Weight update:**\n",
    "$$\n",
    "\\Delta w = \\begin{cases}\n",
    "A_+ e^{-\\Delta t / \\tau_+} & \\text{if } \\Delta t > 0 \\\\\n",
    "-A_- e^{\\Delta t / \\tau_-} & \\text{if } \\Delta t < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where $\\Delta t = t_{post} - t_{pre}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STDP learning window\n",
    "def stdp_window(dt_values, A_plus=0.01, A_minus=0.01, tau_plus=20.0, tau_minus=20.0):\n",
    "    \"\"\"Compute STDP weight change as a function of spike timing difference.\"\"\"\n",
    "    dw = np.zeros_like(dt_values)\n",
    "    \n",
    "    # Potentiation (pre before post)\n",
    "    pos_mask = dt_values > 0\n",
    "    dw[pos_mask] = A_plus * np.exp(-dt_values[pos_mask] / tau_plus)\n",
    "    \n",
    "    # Depression (post before pre)\n",
    "    neg_mask = dt_values < 0\n",
    "    dw[neg_mask] = -A_minus * np.exp(dt_values[neg_mask] / tau_minus)\n",
    "    \n",
    "    return dw\n",
    "\n",
    "# Plot STDP window\n",
    "dt_range = np.linspace(-100, 100, 1000)\n",
    "dw_values = stdp_window(dt_range)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot STDP curve\n",
    "ax.plot(dt_range, dw_values, 'b-', linewidth=3)\n",
    "ax.axhline(0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.axvline(0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Annotate regions\n",
    "ax.fill_between(dt_range[dt_range > 0], 0, dw_values[dt_range > 0], \n",
    "                alpha=0.3, color='green', label='LTP (potentiation)')\n",
    "ax.fill_between(dt_range[dt_range < 0], 0, dw_values[dt_range < 0], \n",
    "                alpha=0.3, color='red', label='LTD (depression)')\n",
    "\n",
    "ax.set_xlabel('Î”t = t_post - t_pre (ms)', fontsize=12)\n",
    "ax.set_ylabel('Weight Change Î”w', fontsize=12)\n",
    "ax.set_title('STDP Learning Window', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "ax.annotate('Pre â†’ Post\\nStrengthen', xy=(30, 0.006), fontsize=10,\n",
    "           ha='center', bbox=dict(boxstyle='round', facecolor='green', alpha=0.2))\n",
    "ax.annotate('Post â†’ Pre\\nWeaken', xy=(-30, -0.006), fontsize=10,\n",
    "           ha='center', bbox=dict(boxstyle='round', facecolor='red', alpha=0.2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š STDP Principle:\")\n",
    "print(\"   'Neurons that fire together, wire together'\")\n",
    "print(\"   Positive Î”t (preâ†’post): Potentiation (LTP)\")\n",
    "print(\"   Negative Î”t (postâ†’pre): Depression (LTD)\")\n",
    "print(\"   Exponential decay with distance from Î”t=0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implementing STDP in Networks\n",
    "\n",
    "Let's implement a simple STDP learning rule in a small network. We'll track spike times and update weights according to the STDP rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDPSynapse(bp.Synapse):\n",
    "    \"\"\"Synapse with STDP learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, size, tau=5.0*u.ms, A_plus=0.01, A_minus=0.01, \n",
    "                 tau_plus=20.0*u.ms, tau_minus=20.0*u.ms, w_max=1.0, **kwargs):\n",
    "        super().__init__(size, **kwargs)\n",
    "        \n",
    "        self.tau = tau\n",
    "        self.A_plus = A_plus\n",
    "        self.A_minus = A_minus\n",
    "        self.tau_plus = tau_plus\n",
    "        self.tau_minus = tau_minus\n",
    "        self.w_max = w_max\n",
    "        \n",
    "        # States\n",
    "        self.g = brainstate.ShortTermState(jnp.zeros(size))\n",
    "        self.w = brainstate.ParamState(jnp.ones(size) * 0.5)  # Learnable weights\n",
    "        self.pre_trace = brainstate.ShortTermState(jnp.zeros(size))  # Pre-synaptic trace\n",
    "        self.post_trace = brainstate.ShortTermState(jnp.zeros(size))  # Post-synaptic trace\n",
    "    \n",
    "    def reset_state(self, batch_size=None):\n",
    "        self.g.value = jnp.zeros(self.size if batch_size is None else (batch_size, self.size))\n",
    "        self.pre_trace.value = jnp.zeros(self.size if batch_size is None else (batch_size, self.size))\n",
    "        self.post_trace.value = jnp.zeros(self.size if batch_size is None else (batch_size, self.size))\n",
    "    \n",
    "    def update(self, pre_spike, post_spike=None):\n",
    "        dt = brainstate.environ.get_dt()\n",
    "        \n",
    "        # Update pre-synaptic trace\n",
    "        self.pre_trace.value += -self.pre_trace.value / self.tau_plus.to_decimal(u.ms) * dt.to_decimal(u.ms)\n",
    "        self.pre_trace.value += pre_spike\n",
    "        \n",
    "        # Update conductance\n",
    "        dg = -self.g.value / self.tau.to_decimal(u.ms) * dt.to_decimal(u.ms)\n",
    "        self.g.value += dg + pre_spike * self.w.value\n",
    "        \n",
    "        # STDP learning (if post spike provided)\n",
    "        if post_spike is not None:\n",
    "            # Update post-synaptic trace\n",
    "            self.post_trace.value += -self.post_trace.value / self.tau_minus.to_decimal(u.ms) * dt.to_decimal(u.ms)\n",
    "            self.post_trace.value += post_spike\n",
    "            \n",
    "            # Weight updates\n",
    "            # LTP: pre spike causes weight increase proportional to post trace\n",
    "            dw_ltp = self.A_plus * pre_spike * self.post_trace.value\n",
    "            # LTD: post spike causes weight decrease proportional to pre trace\n",
    "            dw_ltd = -self.A_minus * post_spike * self.pre_trace.value\n",
    "            \n",
    "            # Update weights with bounds\n",
    "            self.w.value = jnp.clip(self.w.value + dw_ltp + dw_ltd, 0.0, self.w_max)\n",
    "        \n",
    "        return self.g.value\n",
    "\n",
    "# Test STDP learning\n",
    "stdp_syn = STDPSynapse(size=1, A_plus=0.005, A_minus=0.005)\n",
    "brainstate.nn.init_all_states(stdp_syn)\n",
    "\n",
    "# Simulate with correlated pre-post spikes\n",
    "duration = 1000 * u.ms\n",
    "n_steps = int(duration / brainstate.environ.get_dt())\n",
    "\n",
    "# Pre spikes followed by post spikes (should cause LTP)\n",
    "pre_spike_times = [100, 300, 500, 700, 900]  # ms\n",
    "post_spike_times = [105, 305, 505, 705, 905]  # 5ms after pre (potentiation)\n",
    "\n",
    "pre_indices = [int(t / 0.1) for t in pre_spike_times]\n",
    "post_indices = [int(t / 0.1) for t in post_spike_times]\n",
    "\n",
    "w_history = []\n",
    "for i in range(n_steps):\n",
    "    pre_spike = 1.0 if i in pre_indices else 0.0\n",
    "    post_spike = 1.0 if i in post_indices else 0.0\n",
    "    stdp_syn(pre_spike, post_spike)\n",
    "    w_history.append(float(stdp_syn.w.value))\n",
    "\n",
    "# Plot weight evolution\n",
    "times_plot = np.arange(n_steps) * 0.1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(times_plot, w_history, 'b-', linewidth=2, label='Synaptic Weight')\n",
    "\n",
    "for pt, pst in zip(pre_spike_times, post_spike_times):\n",
    "    ax.axvline(pt, color='g', linestyle='--', alpha=0.3, linewidth=1.5)\n",
    "    ax.axvline(pst, color='r', linestyle='--', alpha=0.3, linewidth=1.5)\n",
    "\n",
    "# Add legend entries\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='b', linewidth=2, label='Synaptic Weight'),\n",
    "    Line2D([0], [0], color='g', linestyle='--', label='Pre-spike'),\n",
    "    Line2D([0], [0], color='r', linestyle='--', label='Post-spike (5ms later)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, fontsize=11)\n",
    "\n",
    "ax.set_xlabel('Time (ms)', fontsize=12)\n",
    "ax.set_ylabel('Weight', fontsize=12)\n",
    "ax.set_title('STDP Learning: Weight Potentiation', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸ“Š STDP Learning Result:\")\n",
    "print(f\"   Initial weight: {w_history[0]:.3f}\")\n",
    "print(f\"   Final weight: {w_history[-1]:.3f}\")\n",
    "print(f\"   Change: {w_history[-1] - w_history[0]:+.3f}\")\n",
    "print(f\"   âœ… Weight increased due to consistent preâ†’post timing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Network with Plastic Synapses\n",
    "\n",
    "Let's build a small recurrent network with STDP to see how plasticity affects network dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlasticNetwork(brainstate.nn.Module):\n",
    "    \"\"\"Recurrent network with STDP.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_neurons=10, connectivity=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_neurons = n_neurons\n",
    "        \n",
    "        # LIF neurons\n",
    "        self.neurons = bp.LIF(\n",
    "            n_neurons,\n",
    "            V_rest=-65.0 * u.mV,\n",
    "            V_th=-50.0 * u.mV,\n",
    "            V_reset=-65.0 * u.mV,\n",
    "            tau=10.0 * u.ms\n",
    "        )\n",
    "        \n",
    "        # Recurrent connections with STDP (simplified)\n",
    "        # In practice, use projection structure\n",
    "        self.connectivity = connectivity\n",
    "        mask = (np.random.rand(n_neurons, n_neurons) < connectivity).astype(float)\n",
    "        np.fill_diagonal(mask, 0)  # No self-connections\n",
    "        \n",
    "        self.conn_matrix = brainstate.ParamState(jnp.array(mask))\n",
    "        self.weights = brainstate.ParamState(\n",
    "            jnp.array(mask * 0.5)  # Initial weights\n",
    "        )\n",
    "    \n",
    "    def update(self, inp):\n",
    "        # Get current spikes\n",
    "        spikes = self.neurons.get_spike()\n",
    "        \n",
    "        # Compute recurrent input\n",
    "        recurrent_input = jnp.dot(spikes, self.weights.value) * u.nA\n",
    "        \n",
    "        # Update neurons\n",
    "        self.neurons(inp + recurrent_input)\n",
    "        \n",
    "        return spikes\n",
    "    \n",
    "    def apply_stdp(self, pre_spikes, post_spikes, learning_rate=0.001):\n",
    "        \"\"\"Apply STDP update to weights.\"\"\"\n",
    "        # Simple STDP: strengthen connections where both fire\n",
    "        # (This is simplified; real STDP uses spike timing)\n",
    "        dw = learning_rate * jnp.outer(post_spikes, pre_spikes)\n",
    "        \n",
    "        # Update weights with connectivity mask\n",
    "        new_weights = self.weights.value + dw * self.conn_matrix.value\n",
    "        self.weights.value = jnp.clip(new_weights, 0.0, 1.0)\n",
    "\n",
    "# Create network\n",
    "net = PlasticNetwork(n_neurons=20, connectivity=0.2)\n",
    "brainstate.nn.init_all_states(net)\n",
    "\n",
    "# Simulate with external input\n",
    "duration = 500 * u.ms\n",
    "n_steps = int(duration / brainstate.environ.get_dt())\n",
    "\n",
    "spike_records = []\n",
    "weight_norms = []\n",
    "\n",
    "for i in range(n_steps):\n",
    "    # Random external input\n",
    "    inp = brainstate.random.rand(net.n_neurons) * 2.0 * u.nA\n",
    "    \n",
    "    # Get spikes before update\n",
    "    pre_spikes = net.neurons.get_spike()\n",
    "    \n",
    "    # Update network\n",
    "    post_spikes = net(inp)\n",
    "    \n",
    "    # Apply STDP\n",
    "    if i % 10 == 0:  # Update every 10 steps\n",
    "        net.apply_stdp(pre_spikes, post_spikes)\n",
    "    \n",
    "    spike_records.append(post_spikes)\n",
    "    weight_norms.append(float(jnp.linalg.norm(net.weights.value)))\n",
    "\n",
    "spike_records = jnp.array(spike_records)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Spike raster\n",
    "times_ms = np.arange(n_steps) * 0.1\n",
    "for neuron_idx in range(net.n_neurons):\n",
    "    spike_times = times_ms[spike_records[:, neuron_idx] > 0]\n",
    "    axes[0].scatter(spike_times, [neuron_idx] * len(spike_times), \n",
    "                   s=1, c='black', alpha=0.5)\n",
    "\n",
    "axes[0].set_ylabel('Neuron Index', fontsize=12)\n",
    "axes[0].set_title('Network Activity with STDP', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlim(0, float(duration.to_decimal(u.ms)))\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Weight evolution\n",
    "axes[1].plot(times_ms, weight_norms, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Time (ms)', fontsize=12)\n",
    "axes[1].set_ylabel('Weight Norm', fontsize=12)\n",
    "axes[1].set_title('Evolution of Synaptic Weights', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Network with Plasticity:\")\n",
    "print(f\"   Initial weight norm: {weight_norms[0]:.3f}\")\n",
    "print(f\"   Final weight norm: {weight_norms[-1]:.3f}\")\n",
    "print(f\"   Change: {weight_norms[-1] - weight_norms[0]:+.3f}\")\n",
    "print(\"   Weights adapt based on network activity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Combining Plasticity with Training\n",
    "\n",
    "Plasticity can be combined with gradient-based training. This creates networks that:\n",
    "1. Learn through backpropagation (supervised)\n",
    "2. Adapt through plasticity (unsupervised)\n",
    "\n",
    "**Hybrid approach:**\n",
    "- Use gradient descent to train feedforward weights\n",
    "- Use STDP/STP for recurrent weights\n",
    "- Combine benefits of both learning paradigms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for hybrid learning\n",
    "class HybridNetwork(brainstate.nn.Module):\n",
    "    \"\"\"Network combining gradient-based and plasticity-based learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feedforward layers (trained with gradients)\n",
    "        self.fc1 = brainstate.nn.Linear(n_input, n_hidden)\n",
    "        self.hidden = bp.LIF(\n",
    "            n_hidden,\n",
    "            V_rest=-65.0*u.mV, V_th=-50.0*u.mV, tau=10.0*u.ms,\n",
    "            spike_fun=braintools.surrogate.ReluGrad()\n",
    "        )\n",
    "        \n",
    "        # Recurrent connections (updated with STDP)\n",
    "        # Would use STDPSynapse in practice\n",
    "        \n",
    "        self.fc2 = brainstate.nn.Linear(n_hidden, n_output)\n",
    "        self.output = bp.LIF(\n",
    "            n_output,\n",
    "            V_rest=-65.0*u.mV, V_th=-50.0*u.mV, tau=10.0*u.ms,\n",
    "            spike_fun=braintools.surrogate.ReluGrad()\n",
    "        )\n",
    "        \n",
    "        self.readout = bp.Readout(n_output, n_output)\n",
    "    \n",
    "    def update(self, x):\n",
    "        # Feedforward path (gradient-trained)\n",
    "        current1 = self.fc1(x)\n",
    "        self.hidden(current1)\n",
    "        h_spikes = self.hidden.get_spike()\n",
    "        \n",
    "        # Add recurrent dynamics here (STDP-updated)\n",
    "        # ...\n",
    "        \n",
    "        current2 = self.fc2(h_spikes)\n",
    "        self.output(current2)\n",
    "        o_spikes = self.output.get_spike()\n",
    "        \n",
    "        return self.readout(o_spikes)\n",
    "\n",
    "print(\"ðŸ’¡ Hybrid Learning Strategy:\")\n",
    "print(\"\"\"\\n1. Feedforward weights: Trained with gradient descent (supervised)\n",
    "   - Fast convergence\n",
    "   - Optimized for task objective\n",
    "\n",
    "2. Recurrent weights: Updated with STDP (unsupervised)\n",
    "   - Biologically plausible\n",
    "   - Adapts to input statistics\n",
    "   - Provides temporal dynamics\n",
    "\n",
    "3. Benefits:\n",
    "   - Best of both worlds\n",
    "   - Robust to distribution shift\n",
    "   - Continual adaptation\n",
    "\n",
    "Implementation:\n",
    "   - Train feedforward with brainstate.transform.grad()\n",
    "   - Update recurrent with STDP rule\n",
    "   - Alternate or interleave both updates\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "âœ… **Short-term plasticity (STP)**\n",
    "   - Depression: Resource depletion, decreasing response\n",
    "   - Facilitation: Calcium buildup, increasing response\n",
    "   - Combined dynamics for realistic synapses\n",
    "\n",
    "âœ… **STDP principles**\n",
    "   - Spike timing matters: preâ†’post strengthens, postâ†’pre weakens\n",
    "   - Exponential learning window\n",
    "   - \"Fire together, wire together\"\n",
    "\n",
    "âœ… **Implementation**\n",
    "   - Create custom synapse classes with plasticity\n",
    "   - Track spike traces for STDP\n",
    "   - Update weights based on activity\n",
    "\n",
    "âœ… **Network plasticity**\n",
    "   - Embed plastic synapses in networks\n",
    "   - Observe weight evolution\n",
    "   - Combine with gradient-based training\n",
    "\n",
    "**Key code patterns:**\n",
    "\n",
    "```python\n",
    "# Short-term depression\n",
    "class STDSynapse(bp.Synapse):\n",
    "    def update(self, pre_spike):\n",
    "        # Deplete resources on spike\n",
    "        self.x.value -= pre_spike * U * self.x.value\n",
    "        # Exponential recovery\n",
    "        self.x.value += (1 - self.x.value) / tau_d * dt\n",
    "        # Modulated conductance\n",
    "        self.g.value += pre_spike * U * self.x.value\n",
    "\n",
    "# STDP learning\n",
    "class STDPSynapse(bp.Synapse):\n",
    "    def update(self, pre_spike, post_spike):\n",
    "        # Update traces\n",
    "        self.pre_trace.value += pre_spike\n",
    "        self.post_trace.value += post_spike\n",
    "        # Weight updates\n",
    "        dw_ltp = A_plus * pre_spike * self.post_trace.value\n",
    "        dw_ltd = -A_minus * post_spike * self.pre_trace.value\n",
    "        self.w.value += dw_ltp + dw_ltd\n",
    "```\n",
    "\n",
    "**Next steps:**\n",
    "- Implement full STDP in recurrent networks\n",
    "- Explore homeostatic plasticity (weight normalization)\n",
    "- Combine plasticity with network training (Tutorial 5)\n",
    "- Study biological learning rules (BCM, Oja's rule)\n",
    "- See Tutorial 7 for scaling plastic networks\n",
    "\n",
    "**References:**\n",
    "- Markram et al. (1998): \"Redistribution of synaptic efficacy between neocortical pyramidal neurons\" (STP)\n",
    "- Bi & Poo (1998): \"Synaptic modifications in cultured hippocampal neurons\" (STDP)\n",
    "- Song et al. (2000): \"Competitive Hebbian learning through spike-timing-dependent synaptic plasticity\" (STDP theory)\n",
    "- Tsodyks & Markram (1997): \"The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability\" (STP model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Test your understanding:\n",
    "\n",
    "### Exercise 1: Parameter Exploration\n",
    "Vary STD/STF time constants and observe how they affect frequency filtering. Which regimes amplify or attenuate high-frequency inputs?\n",
    "\n",
    "### Exercise 2: STDP Pattern Learning\n",
    "Create a network that learns to respond to specific temporal patterns using STDP. Test with repeated spike sequences.\n",
    "\n",
    "### Exercise 3: Homeostatic Plasticity\n",
    "Implement weight normalization to prevent runaway potentiation/depression. Keep total synaptic weight constant.\n",
    "\n",
    "### Exercise 4: Recurrent STDP\n",
    "Build a recurrent network where all connections use STDP. Observe emergence of structured connectivity.\n",
    "\n",
    "### Exercise 5: Hybrid Training\n",
    "Combine gradient-based training (Tutorial 5) with STDP in recurrent connections. Compare performance with pure gradient descent.\n",
    "\n",
    "**Bonus Challenge:** Implement triplet STDP, which considers triplets of spikes for more accurate learning rules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
