"""Checkpointing helper functions.

This module is rewritten from the Flax APIs (https://github.com/google/flax).
"""

import enum
import functools
import logging
import os
import pathlib
import re
import shutil
import sys
import threading
import time
import warnings
from concurrent.futures import thread
from contextlib import contextmanager
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union

import jax
import numpy as np
from jax import monitoring
from jax import process_index
from jax.experimental.multihost_utils import sync_global_devices
try:
  from jax.experimental.array_serialization import get_tensorstore_spec, GlobalAsyncCheckpointManager  # noqa
except:
  get_tensorstore_spec = GlobalAsyncCheckpointManager = None

try:
  import msgpack
except ModuleNotFoundError:
  msgpack = None

from brainpy._src.math.ndarray import Array
from brainpy.errors import (AlreadyExistsError,
                            MPACheckpointingRequiredError,
                            MPARestoreTargetRequiredError,
                            MPARestoreDataCorruptedError,
                            InvalidCheckpointPath,
                            InvalidCheckpointError)
from brainpy.types import PyTree

__all__ = [
  # saving
  'save', 'multiprocess_save', 'save_pytree', 'load_pytree',
  # loading
  'load',
  # async
  'AsyncManager',
]

_LAST_CHECKPOINT_WRITE_TIME = time.time()
_READ_CHECKPOINT_EVENT: str = '/jax/checkpoint/read/durations_sec'
_WRITE_CHECKPOINT_EVENT: str = '/jax/checkpoint/write/durations_sec'

# Single-group reg-exps for int or float numerical substrings.
# captures sign:
SIGNED_FLOAT_RE = re.compile(r'([-+]?(?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?)')
# does not capture sign:
UNSIGNED_FLOAT_RE = re.compile(r'[-+]?((?:\d+(?:\.\d*)?|\.\d+)(?:[eE][-+]?\d+)?)')
# Module name followed by number.
MODULE_NUM_RE = re.compile(r'(.*)_\d+$')
# Alternative schemes handled by `gfile`, e.g. on Google Cloud Storage (GCS).
SCHEME_RE = re.compile('^(?P<scheme>[a-z][a-z0-9.+-]+://)?(?P<path>.*)', re.I)

# Multiprocess arrays (GlobalDeviceArray, or JAX array with multiprocess
# sharding) is across processes and will be stored in directories with this
# postfix, seperated from the non-distributed data (e.g. the larger pytree)
MP_ARRAY_POSTFIX = '_gda'
# Occurrences of multiprocess arrays in the target pytree will be
# replaced by this string placeholder.
MP_ARRAY_PH = '//GDAPlaceholder:'

# Add a copy-success file to a distributed array directory to indicate the
# array save is complete.
# We need this for GCS because GCS's directory move is not atomic.
COMMIT_SUCCESS_FILE = 'commit_success.txt'

# Orbax main checkpoint file name.
ORBAX_CKPT_FILENAME = 'checkpoint'

# Chunking array leaves

# msgpack has a hard limit of 2**31 - 1 bytes per object leaf.  To circumvent
# this limit for giant arrays (e.g. embedding tables), we traverse the tree
# and break up arrays near the limit into flattened array chunks.
# True limit is 2**31 - 1, but leave a margin for encoding padding.
MAX_CHUNK_SIZE = 2 ** 30

# containing jax.Array attribute.
MultiprocessArrayType = Any

_STATE_DICT_REGISTRY: Dict[Any, Any] = {}


class _ErrorContext(threading.local):
  """Context for deserialization error messages."""

  def __init__(self):
    self.path = []


_error_context = _ErrorContext()


@contextmanager
def _record_path(name):
  try:
    _error_context.path.append(name)
    yield
  finally:
    _error_context.path.pop()


def check_msgpack():
  if msgpack is None:
    raise ModuleNotFoundError('\nbrainpy.checkpoints needs "msgpack" package. Please install msgpack via:\n'
                              '> pip install msgpack')


def current_path():
  """Current state_dict path during deserialization for error messages."""
  return '/'.join(_error_context.path)


class _NamedTuple:
  """Fake type marker for namedtuple for registry."""
  pass


def _is_namedtuple(x):
  """Duck typing test for namedtuple factory-generated objects."""
  return isinstance(x, tuple) and hasattr(x, '_fields')


def from_state_dict(target, state: Dict[str, Any], name: str = '.'):
  """Restores the state of the given target using a state dict.

  This function takes the current target as an argument. This
  lets us know the exact structure of the target,
  as well as lets us add assertions that shapes and dtypes don't change.

  In practice, none of the leaf values in `target` are actually
  used. Only the tree structure, shapes and dtypes.

  Args:
    target: the object of which the state should be restored.
    state: a dictionary generated by `to_state_dict` with the desired new
           state for `target`.
    name: name of branch taken, used to improve deserialization error messages.
  Returns:
    A copy of the object with the restored state.
  """
  ty = _NamedTuple if _is_namedtuple(target) else type(target)
  for t in _STATE_DICT_REGISTRY.keys():
    if issubclass(ty, t):
      ty = t
      break
  else:
    return state
  ty_from_state_dict = _STATE_DICT_REGISTRY[ty][1]
  with _record_path(name):
    return ty_from_state_dict(target, state)


def to_state_dict(target) -> Dict[str, Any]:
  """Returns a dictionary with the state of the given target."""
  ty = _NamedTuple if _is_namedtuple(target) else type(target)

  for t in _STATE_DICT_REGISTRY.keys():
    if issubclass(ty, t):
      ty = t
      break
  else:
    return target

  ty_to_state_dict = _STATE_DICT_REGISTRY[ty][0]
  state_dict = ty_to_state_dict(target)
  if isinstance(state_dict, dict):
    for key in state_dict.keys():
      assert isinstance(key, str), 'A state dict must only have string keys.'
    return state_dict
  elif isinstance(state_dict, jax.Array):
    return state_dict
  else:
    raise TypeError


def register_serialization_state(ty,
                                 ty_to_state_dict,
                                 ty_from_state_dict,
                                 override=False):
  """Register a type for serialization.

  Args:
    ty: the type to be registered
    ty_to_state_dict: a function that takes an instance of ty and
      returns its state as a dictionary.
    ty_from_state_dict: a function that takes an instance of ty and
      a state dict, and returns a copy of the instance with the restored state.
    override: override a previously registered serialization handler
      (default: False).
  """
  if ty in _STATE_DICT_REGISTRY and not override:
    raise ValueError(f'a serialization handler for "{ty.__name__}"'
                     ' is already registered')
  _STATE_DICT_REGISTRY[ty] = (ty_to_state_dict, ty_from_state_dict)


def _list_state_dict(xs: List[Any]) -> Dict[str, Any]:
  return {str(i): to_state_dict(x) for i, x in enumerate(xs)}


def _restore_list(xs, state_dict: Dict[str, Any]) -> List[Any]:
  if len(state_dict) != len(xs):
    raise ValueError('The size of the list and the state dict do not match,'
                     f' got {len(xs)} and {len(state_dict)} '
                     f'at path {current_path()}')
  ys = []
  for i in range(len(state_dict)):
    y = from_state_dict(xs[i], state_dict[str(i)], name=str(i))
    ys.append(y)
  return ys


def _array_dict_state(x: Array) -> Dict[str, jax.Array]:
  return x.value


def _restore_array(x, state_dict: jax.Array) -> Array:
  x.value = state_dict
  return x


def _dict_state_dict(xs: Dict[str, Any]) -> Dict[str, Any]:
  str_keys = set(str(k) for k in xs.keys())
  if len(str_keys) != len(xs):
    raise ValueError('Dict keys do not have a unique string representation: '
                     f'{str_keys} vs given: {xs}')
  return {str(key): to_state_dict(value) for key, value in xs.items()}


def _restore_dict(xs, states: Dict[str, Any]) -> Dict[str, Any]:
  diff = set(map(str, xs.keys())).difference(states.keys())
  if diff:
    raise ValueError('The target dict keys and state dict keys do not match,'
                     f' target dict contains keys {diff} which are not present in state dict '
                     f'at path {current_path()}')

  return {key: from_state_dict(value, states[str(key)], name=str(key))
          for key, value in xs.items()}


def _namedtuple_state_dict(nt) -> Dict[str, Any]:
  return {key: to_state_dict(getattr(nt, key)) for key in nt._fields}


def _restore_namedtuple(xs, state_dict: Dict[str, Any]):
  """Rebuild namedtuple from serialized dict."""
  if set(state_dict.keys()) == {'name', 'fields', 'values'}:
    state_dict = {state_dict['fields'][str(i)]: state_dict['values'][str(i)]
                  for i in range(len(state_dict['fields']))}

  sd_keys = set(state_dict.keys())
  nt_keys = set(xs._fields)

  if sd_keys != nt_keys:
    raise ValueError('The field names of the state dict and the named tuple do not match,'
                     f' got {sd_keys} and {nt_keys} at path {current_path()}')
  fields = {
    k: from_state_dict(getattr(xs, k), v, name=k)
    for k, v in state_dict.items()
  }
  return type(xs)(**fields)


register_serialization_state(Array, _array_dict_state, _restore_array)
register_serialization_state(dict, _dict_state_dict, _restore_dict)
# register_serialization_state(DotDict, _dict_state_dict, _restore_dict)
# register_serialization_state(Collector, _dict_state_dict, _restore_dict)
# register_serialization_state(ArrayCollector, _dict_state_dict, _restore_dict)
register_serialization_state(list, _list_state_dict, _restore_list)
register_serialization_state(tuple,
                             _list_state_dict,
                             lambda xs, state_dict: tuple(_restore_list(list(xs), state_dict)))
register_serialization_state(_NamedTuple,
                             _namedtuple_state_dict,
                             _restore_namedtuple)
register_serialization_state(
  jax.tree_util.Partial,
  lambda x: {"args": to_state_dict(x.args),
             "keywords": to_state_dict(x.keywords), },
  lambda x, sd: jax.tree_util.Partial(x.func,
                                      *from_state_dict(x.args, sd["args"]),
                                      **from_state_dict(x.keywords, sd["keywords"]))
)


# On-the-wire / disk serialization format

# We encode state-dicts via msgpack, using its custom type extension.
# https://github.com/msgpack/msgpack/blob/master/spec.md
#
# - ndarrays and DeviceArrays are serialized to nested msgpack-encoded string
#   of (shape-tuple, dtype-name (e.g. 'float32'), row-major array-bytes).
#   Note: only simple ndarray types are supported, no objects or fields.
#
# - native complex scalars are converted to nested msgpack-encoded tuples
#   (real, imag).


def _ndarray_to_bytes(arr) -> bytes:
  """Save ndarray to simple msgpack encoding."""
  if isinstance(arr, jax.Array):
    arr = np.array(arr)
  if arr.dtype.hasobject or arr.dtype.isalignedstruct:
    raise ValueError('Object and structured dtypes not supported '
                     'for serialization of ndarrays.')
  tpl = (arr.shape, arr.dtype.name, arr.tobytes('C'))
  return msgpack.packb(tpl, use_bin_type=True)


def _dtype_from_name(name: str):
  """Handle JAX bfloat16 dtype correctly."""
  if name == b'bfloat16':
    return jax.numpy.bfloat16
  else:
    return np.dtype(name)


def _ndarray_from_bytes(data: bytes) -> np.ndarray:
  """Load ndarray from simple msgpack encoding."""
  shape, dtype_name, buffer = msgpack.unpackb(data, raw=True)
  return np.frombuffer(buffer,
                       dtype=_dtype_from_name(dtype_name),
                       count=-1,
                       offset=0).reshape(shape, order='C')


class _MsgpackExtType(enum.IntEnum):
  """Messagepack custom type ids."""
  ndarray = 1
  native_complex = 2
  npscalar = 3


def _msgpack_ext_pack(x):
  """Messagepack encoders for custom types."""
  # TODO: Array here only work when they are fully addressable.
  # If they are not fully addressable, use the GDA path for checkpointing.
  if isinstance(x, (np.ndarray, jax.Array)):
    return msgpack.ExtType(_MsgpackExtType.ndarray, _ndarray_to_bytes(x))
  if np.issctype(type(x)):
    # pack scalar as ndarray
    return msgpack.ExtType(_MsgpackExtType.npscalar,
                           _ndarray_to_bytes(np.asarray(x)))
  elif isinstance(x, complex):
    return msgpack.ExtType(_MsgpackExtType.native_complex,
                           msgpack.packb((x.real, x.imag)))
  return x


def _msgpack_ext_unpack(code, data):
  """Messagepack decoders for custom types."""
  if code == _MsgpackExtType.ndarray:
    return _ndarray_from_bytes(data)
  elif code == _MsgpackExtType.native_complex:
    complex_tuple = msgpack.unpackb(data)
    return complex(complex_tuple[0], complex_tuple[1])
  elif code == _MsgpackExtType.npscalar:
    ar = _ndarray_from_bytes(data)
    return ar[()]  # unpack ndarray to scalar
  return msgpack.ExtType(code, data)


def _np_convert_in_place(d):
  """Convert any jax devicearray leaves to numpy arrays in place."""
  if isinstance(d, dict):
    for k, v in d.items():
      if isinstance(v, jax.Array):
        d[k] = np.array(v)
      elif isinstance(v, dict):
        _np_convert_in_place(v)
  elif isinstance(d, jax.Array):
    return np.array(d)
  return d


_tuple_to_dict = lambda tpl: {str(x): y for x, y in enumerate(tpl)}
_dict_to_tuple = lambda dct: tuple(dct[str(i)] for i in range(len(dct)))


def _chunk(arr) -> Dict[str, Any]:
  """Convert array to a canonical dictionary of chunked arrays."""
  chunksize = max(1, int(MAX_CHUNK_SIZE / arr.dtype.itemsize))
  data = {'__msgpack_chunked_array__': True,
          'shape': _tuple_to_dict(arr.shape)}
  flatarr = arr.reshape(-1)
  chunks = [flatarr[i:i + chunksize] for i in range(0, flatarr.size, chunksize)]
  data['chunks'] = _tuple_to_dict(chunks)
  return data


def _unchunk(data: Dict[str, Any]):
  """Convert canonical dictionary of chunked arrays back into array."""
  assert '__msgpack_chunked_array__' in data
  shape = _dict_to_tuple(data['shape'])
  flatarr = np.concatenate(_dict_to_tuple(data['chunks']))
  return flatarr.reshape(shape)


def _chunk_array_leaves_in_place(d):
  """Convert oversized array leaves to safe chunked form in place."""
  if isinstance(d, dict):
    for k, v in d.items():
      if isinstance(v, np.ndarray):
        if v.size * v.dtype.itemsize > MAX_CHUNK_SIZE:
          d[k] = _chunk(v)
      elif isinstance(v, dict):
        _chunk_array_leaves_in_place(v)
  elif isinstance(d, np.ndarray):
    if d.size * d.dtype.itemsize > MAX_CHUNK_SIZE:
      return _chunk(d)
  return d


def _unchunk_array_leaves_in_place(d):
  """Convert chunked array leaves back into array leaves, in place."""
  if isinstance(d, dict):
    if '__msgpack_chunked_array__' in d:
      return _unchunk(d)
    else:
      for k, v in d.items():
        if isinstance(v, dict) and '__msgpack_chunked_array__' in v:
          d[k] = _unchunk(v)
        elif isinstance(v, dict):
          _unchunk_array_leaves_in_place(v)
  return d


def msgpack_serialize(pytree, in_place: bool = False) -> bytes:
  """Save data structure to bytes in msgpack format.

  Low-level function that only supports python trees with array leaves,
  for custom objects use `to_bytes`.  It splits arrays above MAX_CHUNK_SIZE into
  multiple chunks.

  Args:
    pytree: python tree of dict, list, tuple with python primitives
      and array leaves.
    in_place: boolean specifyng if pytree should be modified in place.

  Returns:
    msgpack-encoded bytes of pytree.
  """
  if not in_place:
    pytree = jax.tree_util.tree_map(lambda x: x, pytree)
  pytree = _np_convert_in_place(pytree)
  pytree = _chunk_array_leaves_in_place(pytree)
  return msgpack.packb(pytree, default=_msgpack_ext_pack, strict_types=True)


def msgpack_restore(encoded_pytree: bytes):
  """Restore data structure from bytes in msgpack format.

  Low-level function that only supports python trees with array leaves,
  for custom objects use `from_bytes`.

  Args:
    encoded_pytree: msgpack-encoded bytes of python tree.

  Returns:
    Python tree of dict, list, tuple with python primitive
    and array leaves.
  """
  state_dict = msgpack.unpackb(
    encoded_pytree, ext_hook=_msgpack_ext_unpack, raw=False)
  return _unchunk_array_leaves_in_place(state_dict)


def from_bytes(target, encoded_bytes: bytes):
  """Restore optimizer or other object from msgpack-serialized state-dict.

  Args:
    target: template object with state-dict registrations that matches
      the structure being deserialized from `encoded_bytes`.
    encoded_bytes: msgpack serialized object structurally isomorphic to
      `target`.  Typically a flax model or optimizer.

  Returns:
    A new object structurally isomorphic to `target` containing the updated
    leaf data from saved data.
  """
  state_dict = msgpack_restore(encoded_bytes)
  return from_state_dict(target, state_dict)


def to_bytes(target) -> bytes:
  """Save optimizer or other object as msgpack-serialized state-dict.

  Args:
    target: template object with state-dict registrations to be
      serialized to msgpack format.  Typically a flax model or optimizer.

  Returns:
    Bytes of msgpack-encoded state-dict of `target` object.
  """
  state_dict = to_state_dict(target)
  return msgpack_serialize(state_dict, in_place=True)


# the empty node is a struct.dataclass to be compatible with JAX.
class _EmptyNode:
  pass


def flatten_dict(xs, keep_empty_nodes=False, is_leaf=None, sep=None):
  """Flatten a nested dictionary.

  The nested keys are flattened to a tuple.
  See `unflatten_dict` on how to restore the
  nested dictionary structure.

  Example::

    xs = {'foo': 1, 'bar': {'a': 2, 'b': {}}}
    flat_xs = flatten_dict(xs)
    print(flat_xs)
    # {
    #   ('foo',): 1,
    #   ('bar', 'a'): 2,
    # }

  Note that empty dictionaries are ignored and
  will not be restored by `unflatten_dict`.

  Args:
    xs: a nested dictionary
    keep_empty_nodes: replaces empty dictionaries
      with `traverse_util.empty_node`.
    is_leaf: an optional function that takes the
      next nested dictionary and nested keys and
      returns True if the nested dictionary is a
      leaf (i.e., should not be flattened further).
    sep: if specified, then the keys of the returned
      dictionary will be `sep`-joined strings (if
      `None`, then keys will be tuples).
  Returns:
    The flattened dictionary.
  """
  assert isinstance(xs, dict), f'expected (frozen)dict; got {type(xs)}'

  def _key(path):
    if sep is None:
      return path
    return sep.join(path)

  def _flatten(xs, prefix):
    if not isinstance(xs, dict) or (is_leaf and is_leaf(prefix, xs)):
      return {_key(prefix): xs}
    result = {}
    is_empty = True
    for key, value in xs.items():
      is_empty = False
      path = prefix + (key,)
      result.update(_flatten(value, path))
    if keep_empty_nodes and is_empty:
      if prefix == ():  # when the whole input is empty
        return {}
      return {_key(prefix): _EmptyNode()}
    return result

  return _flatten(xs, ())


def unflatten_dict(xs, sep=None):
  """Unflatten a dictionary.

  See `flatten_dict`

  Example::

    flat_xs = {
      ('foo',): 1,
      ('bar', 'a'): 2,
    }
    xs = unflatten_dict(flat_xs)
    print(xs)
    # {
    #   'foo': 1
    #   'bar': {'a': 2}
    # }

  Args:
    xs: a flattened dictionary
    sep: separator (same as used with `flatten_dict()`).
  Returns:
    The nested dictionary.
  """
  assert isinstance(xs, dict), f'input is not a dict; it is a {type(xs)}'
  result = {}
  for path, value in xs.items():
    if sep is not None:
      path = path.split(sep)
    if isinstance(value, _EmptyNode):
      value = {}
    cursor = result
    for key in path[:-1]:
      if key not in cursor:
        cursor[key] = {}
      cursor = cursor[key]
    cursor[path[-1]] = value
  return result


def _rename_fn(src, dst, overwrite=False):
  if os.path.exists(src):
    if os.path.exists(dst) and not overwrite:
      raise AlreadyExistsError(dst)
    return os.rename(src, dst)


def _checkpoint_path(ckpt_dir: str,
                     step: Union[int, float, str],
                     prefix: str = 'checkpoint_') -> str:
  return os.path.join(ckpt_dir, f'{prefix}{step}')


def _checkpoint_path_step(path: str) -> Optional[float]:
  """Returns the step number of a checkpoint path."""
  for s in SIGNED_FLOAT_RE.split(path)[::-1]:
    if SIGNED_FLOAT_RE.match(s):
      return float(s)
  return None


def _allowempty_listdir(path: str):
  try:
    return os.listdir(path)
  except FileNotFoundError:
    return []


def _safe_remove(path: str):
  """Identify whether a path is a dir or list and choose the correct remove method."""
  if os.path.isdir(path):
    shutil.rmtree(path)
  else:
    os.remove(path)


class AsyncManager(object):
  """A simple object to track async checkpointing.

  How to use: create an instance and pass to `brainpy.checkpoints.save()` calls:
    am = AsyncManager()
    brainpy.checkpoints.save(..., async_manager=am)
  """

  def __init__(self, max_workers: int = 1):
    self.executor = thread.ThreadPoolExecutor(max_workers=max_workers)
    self.save_future = None

  def wait_previous_save(self):
    """Block until the previous save finishes, to keep files' consistency."""
    if self.save_future and not self.save_future.done():
      warnings.warn(
        'The previous async brainpy.checkpoints.save has not finished yet. Waiting '
        'for it to complete before the next save.',
        UserWarning
      )
      self.save_future.result()

  def save_async(self, task: Callable[[], Any]):
    """Run a task async. The future will be tracked as self.save_future.

    Args:
      task: The callable to be executed asynchrously.
    """
    self.wait_previous_save()
    self.save_future = self.executor.submit(task)  # type: ignore


def _use_multiprocess_serialization(value: Any) -> bool:
  """Use GlobalAsyncCheckpointManager to save the array if it's only partially available on this host."""
  if isinstance(value, jax.Array):
    return not value.is_fully_addressable
  return False


def _split_mp_arrays(
    target: Dict[str, Any]
) -> Tuple[Dict[str, Any], List[Tuple[MultiprocessArrayType, str]]]:
  """Split out the multiprocess arrays from the target pytree to save."""
  # When target is a single leaf instead of a pytree dict.
  if not isinstance(target, dict):
    if _use_multiprocess_serialization(target):
      return MP_ARRAY_PH, [(target, '')]
    return target, []
  # Traverse the target and handle distributed arrays.
  flattened = flatten_dict(target, keep_empty_nodes=True)
  mpa_targets = []
  for key, value in flattened.items():
    if _use_multiprocess_serialization(value):
      subpath = '/'.join(key)
      mpa_targets.append((value, subpath))
      flattened[key] = MP_ARRAY_PH + subpath
  target = unflatten_dict(flattened)
  return target, mpa_targets


def _make_mpa_dirs(
    mpa_targets: List[Tuple[MultiprocessArrayType, str]],
    tmp_path: str
):
  # Temporary array path is not used in GCS.
  if tmp_path.startswith('gs://'):
    return
  mpa_tmp_path = tmp_path + MP_ARRAY_POSTFIX
  # Clean up the previous MPA dir, in case some leftover from last preemption
  # lingers.
  if os.path.exists(mpa_tmp_path):
    warnings.warn('Removing outdated MPA temporary files at %s' % mpa_tmp_path, UserWarning)
    shutil.rmtree(mpa_tmp_path)
  _, mpa_subpaths = zip(*mpa_targets)
  for subpath in mpa_subpaths:
    os.makedirs(os.path.join(mpa_tmp_path, subpath), exist_ok=True)


def _save_mpas(gda_manager,
               mpa_targets: List[Tuple[MultiprocessArrayType, str]],
               tmp_path: str,
               final_path: str,
               base_path: str,
               keep: int,
               overwrite: bool,
               keep_every_n_steps: Optional[int],
               ckpt_start_time: float,
               async_manager: Optional[AsyncManager] = None):
  """Save the multiprocess arrays given the paths."""
  mpa_list, mpa_subpaths = zip(*mpa_targets)
  mpa_tmp_path, mpa_final_path = tmp_path + MP_ARRAY_POSTFIX, final_path + MP_ARRAY_POSTFIX
  write_commit_success = False
  # If the checkpoint directory is a GCS directory, then keep the final
  # checkpoint directory as the temporary checkpoint directory. This is because
  # renames are not atomic on GCS. When restoring check for the existence of a
  # success file.
  # TODO: figure out a way to unit-test the behavior.
  if tmp_path.startswith('gs://'):
    mpa_tmp_path = mpa_final_path
    write_commit_success = True
  mpa_paths = [os.path.join(mpa_tmp_path, x) for x in mpa_subpaths]
  ts_specs = [get_tensorstore_spec(x) for x in mpa_paths]
  gda_manager.serialize(
    list(mpa_list),
    ts_specs,
    on_commit_callback=functools.partial(
      _save_commit,
      tmp_path,
      final_path,
      base_path,
      keep,
      overwrite,
      keep_every_n_steps,
      ckpt_start_time,
      has_mpa=True,
      write_commit_success=write_commit_success,
      async_manager=async_manager))


def _restore_mpas(state_dict,
                  target: Optional[Any],
                  ckpt_path: str,
                  step: Optional[Union[int, float]],
                  gda_manager: Optional[Any],
                  allow_partial: bool = False):
  """Restore the multiprocess arrays given the target structure and type."""

  def _check_mpa_errors():
    if not gda_manager:
      raise MPACheckpointingRequiredError(ckpt_path, step)
    if not target and not allow_partial:
      raise MPARestoreTargetRequiredError(ckpt_path, step)

  def _safe_deserialize(
      target_mpas: List[Tuple[Tuple[Any, ...], MultiprocessArrayType, str]],
      gda_manager: Any
  ) -> List[MultiprocessArrayType]:
    gda_manager.wait_until_finished()

    # Check if reading from GCS and the array dir is potentially corrupted.
    if ckpt_path.startswith('gs://') and not os.path.exists(
        os.path.join(ckpt_path + MP_ARRAY_POSTFIX, COMMIT_SUCCESS_FILE)):
      raise MPARestoreDataCorruptedError(step, ckpt_path)

    # Check if the given target array types are valid.
    shardings = []
    for _, arr, path in target_mpas:
      if isinstance(arr, jax.Array):
        shardings.append(arr.sharding)

    # Restore the arrays.
    ts_specs = [get_tensorstore_spec(path) for _, _, path in target_mpas]
    return gda_manager.deserialize(shardings, ts_specs)

  # When target is a single leaf instead of a pytree dict.
  if not isinstance(state_dict, dict):
    if (_use_multiprocess_serialization(target) and
        isinstance(state_dict, str) and
        state_dict.startswith(MP_ARRAY_PH)):
      _check_mpa_errors()
      return _safe_deserialize([((), target, ckpt_path + MP_ARRAY_POSTFIX)], gda_manager)[0]
    return state_dict

  # Go through the restored checkpoint pytree for all MPAs
  flattened = flatten_dict(state_dict, keep_empty_nodes=True)
  if target:
    target_flattened = flatten_dict(to_state_dict(target), keep_empty_nodes=True)
  # A list of (state_dict_key, target_array, array_file_path) for every array
  # to be restored
  target_mpas = []
  for key, value in flattened.items():
    if isinstance(value, str) and value.startswith(MP_ARRAY_PH):
      _check_mpa_errors()
      if (not target or
          (key not in target_flattened) or
          (not _use_multiprocess_serialization(target_flattened[key]))):
        if allow_partial:
          warnings.warn(f'Multiprocess array {key} could not be restored '
                        'because a valid array is not found in target at '
                        'the corresponding location. Proceed to restore '
                        'other arrays because allow_partial_restoration=True',
                        UserWarning)
        else:
          raise MPARestoreTargetRequiredError(ckpt_path, step, key)
      else:
        mpa_path = os.path.join(ckpt_path + MP_ARRAY_POSTFIX, value[len(MP_ARRAY_PH):])
        target_mpas.append((key, target_flattened[key], mpa_path))

  # If any MPA needs to be restored, call deserialize
  if target_mpas:
    mpa_list = _safe_deserialize(target_mpas, gda_manager)
    for mpa, (key, _, _) in zip(mpa_list, target_mpas):
      flattened[key] = mpa
    state_dict = unflatten_dict(flattened)
  return state_dict


def _natural_sort(file_list: Iterable[str],
                  signed: bool = True) -> List[str]:
  """Natural sort for filenames with numerical substrings.

  Args:
    file_list: list of paths to sort containing numerical substrings.
    signed: bool: if leading '-' (or '+') signs should be included in numerical
      substrings as a sign or treated as a separator.

  Returns:
    List of filenames sorted 'naturally', not lexicographically: any
    integer substrings are used to subsort numerically. e.g.
    file_1, file_10, file_2  -->  file_1, file_2, file_10
    file_0.1, file_-0.2, file_2.0  -->  file_-0.2, file_0.1, file_2.0
  """
  float_re = SIGNED_FLOAT_RE if signed else UNSIGNED_FLOAT_RE

  def maybe_num(s):
    if float_re.match(s):
      return float(s)
    else:
      return s

  def split_keys(s):
    return [maybe_num(c) for c in float_re.split(s)]

  return sorted(file_list, key=split_keys)


def _safe_normpath(path: str) -> str:
  """Normalizes path safely to get around `io.glob()` limitations."""
  match = SCHEME_RE.match(path)
  assert match is not None
  d = match.groupdict()
  return (d['scheme'] or '') + os.path.normpath(d['path'])


def _remove_invalid_ckpts(ckpt_path: str,
                          base_path: str,
                          keep: int,
                          overwrite: bool,
                          keep_every_n_steps: Optional[int],
                          has_mpa: bool) -> None:
  """Clean up the checkpoint space according to `overwrite`, `keep`, and `keep_every_n_steps` parameters.
  """
  dir_path, prefix = os.path.split(base_path)
  checkpoint_files: List[Any] = [pathlib.PurePath(c) for c in os.listdir(dir_path)]
  checkpoint_files = [
    os.path.join(dir_path, c)
    for c in checkpoint_files
    if c.match(f'{prefix}*') and not c.match(f'*{MP_ARRAY_POSTFIX}')
  ]
  checkpoint_files = _natural_sort(checkpoint_files)

  # Remove newer checkpoints
  if overwrite and ckpt_path in checkpoint_files:
    ind = checkpoint_files.index(ckpt_path) + 1
    newer_ckpts = checkpoint_files[ind:]
    checkpoint_files = checkpoint_files[:ind]
    for path in newer_ckpts:
      warnings.warn(f'Removing checkpoint at {path}', UserWarning)
      if has_mpa:
        # MPA might be removed already but the main ckpt is still there. This
        # can happen if the job is previously preempted after deleting the MPA
        # checkpoint folder and before deleting the main checkpoint.
        if os.path.exists(path + MP_ARRAY_POSTFIX):
          shutil.rmtree(path + MP_ARRAY_POSTFIX)
      _safe_remove(path)

  # Remove old checkpoint files.
  last_kept = -float('inf')
  if len(checkpoint_files) > keep:
    old_ckpts = checkpoint_files[:-keep]
    # Note: old_ckpts is sorted from oldest to newest.
    for path in old_ckpts:
      if keep_every_n_steps:
        step_number = _checkpoint_path_step(path)
        if step_number and (step_number - last_kept) >= keep_every_n_steps:
          logging.debug('Not deleting %s, because last_kept=%f and keeping '
                        'every %d steps.',
                        path, last_kept, keep_every_n_steps)
          last_kept = step_number
          continue
      logging.info('Removing checkpoint at %s', path)
      if has_mpa:
        # MPA might be removed already but the main ckpt is still there.
        if os.path.exists(path + MP_ARRAY_POSTFIX):
          shutil.rmtree(path + MP_ARRAY_POSTFIX)
      _safe_remove(path)


def _record_saved_duration(checkpoint_start_time: float):
  """Record program duration that is accounted for by this checkpoint.

  For the very first checkpoint, this is the interval between program init and
  current checkpoint start time.

  Note that we use the checkpoint start time instead of end time. The saved
  duration should not include prallel training duration while the async
  checkpoint is being written in the background.

  Args:
    checkpoint_start_time: Start time of current checkpoint.
  """
  global _LAST_CHECKPOINT_WRITE_TIME
  # Note: for the very first checkpoint, this is the interval between program
  # init and the current checkpoint start time.
  duration_since_last_checkpoint = checkpoint_start_time - _LAST_CHECKPOINT_WRITE_TIME
  if monitoring is not None:
    monitoring.record_event_duration_secs(
      '/jax/checkpoint/write/duration_since_last_checkpoint_secs',
      duration_since_last_checkpoint)
  _LAST_CHECKPOINT_WRITE_TIME = checkpoint_start_time


def _save_commit(ckpt_tmp_path: str,
                 ckpt_path: str,
                 base_path: str,
                 keep: int,
                 overwrite: bool,
                 keep_every_n_steps: Optional[int],
                 ckpt_start_time: float,
                 has_mpa: bool,
                 write_commit_success: bool,
                 async_manager: Optional[AsyncManager] = None) -> None:
  """Commit changes after saving checkpoints to disk.

  This function does the following, sequentially:
    1. Make sure all ckpt writing finishes, and rename them from temp path to
    the final path.
    2. Remove newer checkpoints (files that ordered larger than this save) if
    `overwrite=True`.
    3. Remove old checkpoint files based on `keep` and `keep_every_n_steps`.
    4. Record program duration saved by this checkpoint.
  """
  mpa_ckpt_tmp_path, mpa_ckpt_path = ckpt_tmp_path + MP_ARRAY_POSTFIX, ckpt_path + MP_ARRAY_POSTFIX
  # Rename the multiprocess array path once serialization and writing finished.
  if has_mpa:
    if write_commit_success:
      commit_success_path = os.path.join(mpa_ckpt_path, COMMIT_SUCCESS_FILE)
      with open(commit_success_path, 'w', encoding='utf-8') as f:
        f.write(f'Checkpoint commit was successful to {mpa_ckpt_path}')
    else:
      # Commits are a two stage process (renaming the array folder and renaming
      # the main ckpt file in sequential order). We always try to overwrite
      # here because the array ckpt might be already renamed in a previously
      # interrupted commit. NOTE: io.rename does not support overwriting
      # directories via `rename` so we manually overwrite it.
      if os.path.exists(mpa_ckpt_path):
        logging.info('Removing outdated checkpoint at %s', mpa_ckpt_path)
        shutil.rmtree(mpa_ckpt_path)
      _rename_fn(mpa_ckpt_tmp_path, mpa_ckpt_path)
  # Commit the main checkpoint file after arrays (if any) are committed
  if async_manager:
    async_manager.wait_previous_save()
  _rename_fn(ckpt_tmp_path, ckpt_path, overwrite=overwrite)
  logging.info('Saved checkpoint at %s', ckpt_path)

  # Remove newer and older invalid checkpoints.
  _remove_invalid_ckpts(ckpt_path, base_path, keep, overwrite,
                        keep_every_n_steps, has_mpa)
  _record_saved_duration(ckpt_start_time)


def _check_overwrite_error(ckpt_tmp_path: str,
                           ckpt_path: str,
                           base_path: str,
                           step: int):
  """Throw error if a ckpt file of this step or higher already exists."""
  dir_path, prefix = os.path.split(base_path)
  checkpoint_files: List[Any] = [pathlib.PurePath(c) for c in _allowempty_listdir(dir_path)]
  checkpoint_files = [
    os.path.join(dir_path, c)
    for c in checkpoint_files
    if c.match(f'{prefix}*') and not c.match(f'*{MP_ARRAY_POSTFIX}')
  ]
  if ckpt_path in checkpoint_files:
    raise InvalidCheckpointError(ckpt_path, step)
  checkpoint_files.append(ckpt_path)

  checkpoint_files = _natural_sort(checkpoint_files)
  # Handle the case if the job was preempted after the temporary checkpoint
  # was written, but before it was renamed to the final checkpoint name
  if checkpoint_files[-1] == ckpt_tmp_path:
    checkpoint_files.pop()
  if ckpt_path != checkpoint_files[-1]:
    raise InvalidCheckpointError(ckpt_path, step)


def _save_main_ckpt_file(target: bytes, has_mpa: bool, paths: Tuple[str, str],
                         base_path: str, step: int,
                         keep: int, overwrite: bool,
                         keep_every_n_steps: Optional[int],
                         ckpt_start_time: float):
  """Save the main checkpoint file via file system."""
  ckpt_tmp_path, ckpt_path = paths
  os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)

  with open(ckpt_tmp_path, 'wb') as fp:
    fp.write(target)

  # Postpone the commitment of checkpoint to after MPA writes are done.
  if not has_mpa:
    _save_commit(
      ckpt_tmp_path,
      ckpt_path,
      base_path,
      keep,
      overwrite,
      keep_every_n_steps,
      ckpt_start_time,
      has_mpa=False,
      write_commit_success=False)


def _get_checkpoint_paths(
    ckpt_dir: Union[str, os.PathLike],
    step: Union[int, float],
    prefix: str = 'checkpoint_'
) -> Tuple[str, str, str]:
  """Generate the checkpoint paths used in this save operation."""
  ckpt_dir = os.fspath(ckpt_dir)  # Pathlib -> str
  logging.info('Saving checkpoint at step: %s', step)
  # normalize path because io.glob() can modify path './', '//' ...
  ckpt_dir = _safe_normpath(ckpt_dir)
  ckpt_tmp_path = _checkpoint_path(ckpt_dir, 'tmp', prefix)
  ckpt_path = _checkpoint_path(ckpt_dir, step, prefix)
  base_path = os.path.join(ckpt_dir, prefix)
  return ckpt_path, ckpt_tmp_path, base_path


def save(
    ckpt_dir: Union[str, os.PathLike],
    target: PyTree,
    step: Union[int, float],
    prefix: str = 'checkpoint_',
    keep: int = 1,
    overwrite: bool = False,
    keep_every_n_steps: Optional[int] = None,
    async_manager: Optional[AsyncManager] = None,
) -> str:
  """Save a checkpoint of the model. Suitable for single-host.

  In this method, every JAX process saves the checkpoint on its own. Do not
  use it if you have multiple processes and you intend for them to save data
  to a common directory (e.g., a GCloud bucket). To save multi-process
  checkpoints to a shared storage or to save `GlobalDeviceArray`s, use
  `multiprocess_save()` instead.

  Pre-emption safe by writing to temporary before a final rename and cleanup
  of past files. However, if async_manager is used, the final
  commit will happen inside an async callback, which can be explicitly waited
  by calling `async_manager.wait_previous_save()`.

  Parameters
  ----------
  ckpt_dir: str, PathLike
    str or pathlib-like path to store checkpoint files in.
  target: Any
    serializable flax object, usually a flax optimizer.
  step: int, float
    training step number or other metric number.
  prefix: str
    checkpoint file name prefix.
  keep: int
    number of past checkpoint files to keep.
  overwrite: bool
    overwrite existing checkpoint files if a checkpoint at the
    current or a later step already exits (default: False).
  keep_every_n_steps: int
    if defined, keep every checkpoints every n steps (in
    addition to keeping the last 'keep' checkpoints).
  async_manager: optional, AsyncManager
    if defined, the save will run without blocking the main
    thread. Only works for single host. Note that an ongoing save will still
    block subsequent saves, to make sure overwrite/keep logic works correctly.

  Returns
  -------
  out: str
    Filename of saved checkpoint.
  """
  check_msgpack()
  start_time = time.time()
  # Make sure all saves are finished before the logic of checking and removing
  # outdated checkpoints happens.
  if async_manager:
    async_manager.wait_previous_save()

  ckpt_path, ckpt_tmp_path, base_path = _get_checkpoint_paths(
    ckpt_dir, step, prefix
  )

  if not overwrite:
    _check_overwrite_error(ckpt_tmp_path, ckpt_path, base_path, step)  # type: ignore

  target = to_bytes(target)

  # Save the files via I/O sync or async.
  def save_main_ckpt_task():
    return _save_main_ckpt_file(target, False, (ckpt_tmp_path, ckpt_path),
                                base_path, step, keep, overwrite,
                                keep_every_n_steps, start_time)

  if async_manager:
    async_manager.save_async(save_main_ckpt_task)
  else:
    save_main_ckpt_task()
  end_time = time.time()
  if monitoring is not None:
    monitoring.record_event_duration_secs(_WRITE_CHECKPOINT_EVENT,
                                          end_time - start_time)
  return ckpt_path


def _save_commit2(filename: str,
                  overwrite: bool,
                  ckpt_start_time: float,
                  has_mpa: bool,
                  write_commit_success: bool,
                  async_manager: Optional[AsyncManager] = None) -> None:
  """Commit changes after saving checkpoints to disk.

  This function does the following, sequentially:
    1. Make sure all ckpt writing finishes, and rename them from temp path to
    the final path.
    2. Remove newer checkpoints (files that ordered larger than this save) if
    `overwrite=True`.
    3. Remove old checkpoint files based on `keep` and `keep_every_n_steps`.
    4. Record program duration saved by this checkpoint.
  """
  ckpt_path = os.path.dirname(filename)
  ckpt_tmp_path = os.path.join(ckpt_path, 'tmp')
  mpa_ckpt_tmp_path, mpa_ckpt_path = ckpt_tmp_path + MP_ARRAY_POSTFIX, ckpt_path + MP_ARRAY_POSTFIX
  # Rename the multiprocess array path once serialization and writing finished.
  if has_mpa:
    if write_commit_success:
      commit_success_path = os.path.join(mpa_ckpt_path, COMMIT_SUCCESS_FILE)
      with open(commit_success_path, 'w', encoding='utf-8') as f:
        f.write(f'Checkpoint commit was successful to {mpa_ckpt_path}')
    else:
      # Commits are a two stage process (renaming the array folder and renaming
      # the main ckpt file in sequential order). We always try to overwrite
      # here because the array ckpt might be already renamed in a previously
      # interrupted commit. NOTE: io.rename does not support overwriting
      # directories via `rename` so we manually overwrite it.
      if os.path.exists(mpa_ckpt_path):
        logging.info('Removing outdated checkpoint at %s', mpa_ckpt_path)
        shutil.rmtree(mpa_ckpt_path)
      _rename_fn(mpa_ckpt_tmp_path, mpa_ckpt_path)
  # Commit the main checkpoint file after arrays (if any) are committed
  if async_manager:
    async_manager.wait_previous_save()
  _rename_fn(ckpt_tmp_path, ckpt_path, overwrite=overwrite)
  logging.info('Saved checkpoint at %s', ckpt_path)

  # Remove newer and older invalid checkpoints.
  _record_saved_duration(ckpt_start_time)


def _save_main_ckpt_file2(target: bytes,
                          has_mpa: bool,
                          filename: str,
                          overwrite: bool,
                          ckpt_start_time: float):
  """Save the main checkpoint file via file system."""
  with open(filename, 'wb') as fp:
    fp.write(target)
  # Postpone the commitment of checkpoint to after MPA writes are done.
  if not has_mpa:
    _save_commit2(filename, overwrite, ckpt_start_time, has_mpa=False, write_commit_success=False)


def save_pytree(
    filename: str,
    target: PyTree,
    overwrite: bool = True,
    async_manager: Optional[AsyncManager] = None,
    verbose: bool = True,
) -> None:
  """Save a checkpoint of the model. Suitable for single-host.

  In this method, every JAX process saves the checkpoint on its own. Do not
  use it if you have multiple processes and you intend for them to save data
  to a common directory (e.g., a GCloud bucket). To save multi-process
  checkpoints to a shared storage or to save `GlobalDeviceArray`s, use
  `multiprocess_save()` instead.

  Pre-emption safe by writing to temporary before a final rename and cleanup
  of past files. However, if async_manager is used, the final
  commit will happen inside an async callback, which can be explicitly waited
  by calling `async_manager.wait_previous_save()`.

  Parameters
  ----------
  filename: str
    str or pathlib-like path to store checkpoint files in.
  target: Any
    serializable flax object, usually a flax optimizer.
  overwrite: bool
    overwrite existing checkpoint files if a checkpoint at the
    current or a later step already exits (default: False).
  async_manager: optional, AsyncManager
    if defined, the save will run without blocking the main
    thread. Only works for single host. Note that an ongoing save will still
    block subsequent saves, to make sure overwrite/keep logic works correctly.
  verbose: bool
    Whether output the print information.

  Returns
  -------
  out: str
    Filename of saved checkpoint.
  """
  check_msgpack()
  if verbose:
    print(f'Saving checkpoint into {filename}')
  start_time = time.time()
  # Make sure all saves are finished before the logic of checking and removing
  # outdated checkpoints happens.
  if async_manager:
    async_manager.wait_previous_save()

  if os.path.splitext(filename)[-1] != '.bp':
    filename = filename + '.bp'
  if os.path.dirname(filename):
    os.makedirs(os.path.dirname(filename), exist_ok=True)
  if not overwrite and os.path.exists(filename):
    raise InvalidCheckpointPath(filename)
  target = to_bytes(target)

  # Save the files via I/O sync or async.
  def save_main_ckpt_task():
    return _save_main_ckpt_file2(target, False, filename, overwrite, start_time)

  if async_manager:
    async_manager.save_async(save_main_ckpt_task)
  else:
    save_main_ckpt_task()
  end_time = time.time()
  if monitoring is not None:
    monitoring.record_event_duration_secs(_WRITE_CHECKPOINT_EVENT,
                                          end_time - start_time)


def multiprocess_save(
    ckpt_dir: Union[str, os.PathLike],
    target: PyTree,
    step: Union[int, float],
    prefix: str = 'checkpoint_',
    keep: int = 1,
    overwrite: bool = False,
    keep_every_n_steps: Optional[int] = None,
    async_manager: Optional[AsyncManager] = None,
    gda_manager: Optional[Any] = None,
) -> str:
  """Save a checkpoint of the model in multi-process environment.

  Use this method to save `GlobalDeviceArray`s, or to save data to a
  common directory. Only process 0 will save the main checkpoint file and
  remove old checkpoint files.

  Pre-emption safe by writing to temporary before a final rename and cleanup
  of past files. However, if async_manager or gda_manager is used, the final
  commit will happen inside an async callback, which can be explicitly waited
  by calling `async_manager.wait_previous_save()` or
  `gda_manager.wait_until_finished()`.

  Parameters
  ----------
  ckpt_dir: str, PathLike
    str or pathlib-like path to store checkpoint files in.
  target: Any
    serializable flax object, usually a flax optimizer.
  step: int, float
    training step number or other metric number.
  prefix: str
    checkpoint file name prefix.
  keep: int
    number of past checkpoint files to keep.
  overwrite: bool
    overwrite existing checkpoint files if a checkpoint at the
    current or a later step already exits (default: False).
  keep_every_n_steps: int
    if defined, keep every checkpoints every n steps (in
    addition to keeping the last 'keep' checkpoints).
  async_manager: optional, AsyncManager
    if defined, the save will run without blocking the main
    thread. Only works for single host. Note that an ongoing save will still
    block subsequent saves, to make sure overwrite/keep logic works correctly.
  gda_manager: optional, Any
    required if target contains a JAX GlobalDeviceArray. Type
    should be GlobalAsyncCheckpointManager (needs Tensorstore to be imported
    correctly). Will save the GDAs to a separate subdirectory with postfix
    "_gda" asynchronously. Same as async_manager, this will block subsequent
    saves.

  Returns
  -------
  out: str
    Filename of saved checkpoint.
  """
  check_msgpack()
  start_time = time.time()
  # Make sure all saves are finished before the logic of checking and removing
  # outdated checkpoints happens.
  sync_global_devices('starting_save_checkpoint')
  if async_manager:
    async_manager.wait_previous_save()
  if gda_manager:
    gda_manager.wait_until_finished()
    sync_global_devices('before_save_checkpoint')

  ckpt_path, ckpt_tmp_path, base_path = _get_checkpoint_paths(
    ckpt_dir, step, prefix)

  target = to_state_dict(target)
  target, mpa_targets = _split_mp_arrays(target)
  target = msgpack_serialize(target)
  has_mpa = len(mpa_targets) > 0 and (get_tensorstore_spec is not None)

  if not overwrite:
    _check_overwrite_error(ckpt_tmp_path, ckpt_path, base_path, step)  # type: ignore
    sync_global_devices('check_overwrite_strictly_before_save')

  # Save the files via I/O sync or async.
  def save_main_ckpt_task():
    return _save_main_ckpt_file(target, has_mpa, (ckpt_tmp_path, ckpt_path),
                                base_path, step, keep, overwrite,
                                keep_every_n_steps, start_time)

  # Write the main checkpoint file only via process 0, to avoid race condition.
  if process_index() == 0:
    if async_manager:
      async_manager.save_async(save_main_ckpt_task)
    else:
      save_main_ckpt_task()

  if has_mpa:
    if not gda_manager:
      raise MPACheckpointingRequiredError(ckpt_path, step)
    # Creating the directory containing GDAs explicitly. This should happen only
    # on process 0 and before any worker starts to write GDA data.
    if process_index() == 0:
      _make_mpa_dirs(mpa_targets, ckpt_tmp_path)
    sync_global_devices('Flax:Checkpointing:AfterCreateMPADir')
    _save_mpas(gda_manager, mpa_targets, ckpt_tmp_path, ckpt_path, base_path,
               keep, overwrite, keep_every_n_steps, start_time, async_manager)

  end_time = time.time()
  if monitoring is not None:
    monitoring.record_event_duration_secs(_WRITE_CHECKPOINT_EVENT,
                                          end_time - start_time)
  return ckpt_path


def _load_latest_fn(
    ckpt_dir: Union[str, os.PathLike],
    prefix: str = 'checkpoint_'
) -> Optional[str]:
  """Retrieve the path of the latest checkpoint in a directory.

  Parameters
  ----------
  ckpt_dir: str
    directory of checkpoints to restore from.
  prefix: str
    name prefix of checkpoint files.

  Returns
  -------
  out: PyTree, None
    The latest checkpoint path or None if no checkpoints were found.
  """
  ckpt_dir = os.fspath(ckpt_dir)  # Pathlib -> str
  checkpoint_files: List[Any] = [
    pathlib.PurePath(c) for c in _allowempty_listdir(ckpt_dir)
  ]
  checkpoint_files = [
    os.path.join(ckpt_dir, c)
    for c in checkpoint_files
    if (c.match(f'{prefix}*') and not c.match(f'{prefix}tmp') and
        not c.match(f'*{MP_ARRAY_POSTFIX}'))
  ]
  checkpoint_files = _natural_sort(checkpoint_files)
  if checkpoint_files:
    return checkpoint_files[-1]
  else:
    return None


def load(
    ckpt_dir: Union[str, os.PathLike],
    step: Optional[Union[int, float]] = None,
    prefix: str = 'checkpoint_',
    target: Optional[Any] = None,
    parallel: bool = True,
    gda_manager: Optional[Any] = None,
    allow_partial_mpa_restoration: bool = False,
) -> PyTree:
  """Load last or best checkpoint from the given checkpoint path.

  Sorts the checkpoint files naturally, returning the highest-valued
  file, e.g.:

  *  ``ckpt_1, ckpt_2, ckpt_3 --> ckpt_3``

  *  ``ckpt_0.01, ckpt_0.1, ckpt_0.001 --> ckpt_0.1``

  *  ``ckpt_-1.0, ckpt_1.0, ckpt_1e5 --> ckpt_1e5``

  Parameters
  ----------
  ckpt_dir: str
    checkpoint file or directory of checkpoints to restore from.
  step: int, float
    Step number to load or None to load latest. If
    specified, ckpt_dir must be a directory.
  prefix: str
    name prefix of checkpoint files.
  target: Any
    Matching object to rebuild via deserialized state-dict. If None, the
    deserialized state-dict is returned as-is.
  parallel: bool
    whether to load seekable checkpoints in parallel, for speed.
  gda_manager: Any
    required if checkpoint contains a multiprocess array
    (GlobalDeviceArray or jax Array from pjit). Type should be
    GlobalAsyncCheckpointManager (needs Tensorstore to be imported
    correctly). Will read the arrays from the separate subdirectory with
    postfix "_gda".
  allow_partial_mpa_restoration: bool
    If true, the given `target` doesn't have to
    contain all valid multiprocess arrays. As a result, the restored Pytree
    may have some MPAs not restored correctly. Use this if you cannot provide
    a fully valid ``target`` and don't need all the MPAs in the checkpoint
    to be restored.

  Returns
  -------
  out: Any
    Restored `target` updated from checkpoint file, or if no step specified and
    no checkpoint files present, returns the passed-in `target` unchanged.
    If a file path is specified and is not found, the passed-in `target` will be
    returned. This is to match the behavior of the case where a directory path
    is specified but the directory has not yet been created.
  """
  check_msgpack()
  start_time = time.time()

  ckpt_dir = os.fspath(ckpt_dir)  # Pathlib -> str
  ckpt_dir = _safe_normpath(ckpt_dir)
  if step:
    ckpt_path = _checkpoint_path(ckpt_dir, step, prefix)
    if not os.path.exists(ckpt_path):
      raise ValueError(f'Matching checkpoint not found: {ckpt_path}')
  else:
    if not os.path.exists(ckpt_dir):
      logging.info('Found no checkpoint directory at %s', ckpt_dir)
      return target
    if os.path.isdir(ckpt_dir):
      # This means the given dir is an orbax checkpoint.
      if os.path.exists(os.path.join(ckpt_dir, ORBAX_CKPT_FILENAME)):
        ckpt_path = ckpt_dir
      else:
        ckpt_path = _load_latest_fn(ckpt_dir, prefix)  # type: ignore
        if not ckpt_path:
          warnings.warn(f'Found no checkpoint files in {ckpt_dir} with prefix {prefix}',
                        UserWarning)
          return target
    else:
      ckpt_path = ckpt_dir

  sys.stdout.write(f'Loading checkpoint from {ckpt_path}\n')
  sys.stdout.flush()
  file_size = os.path.getsize(ckpt_path)

  with open(ckpt_path, 'rb') as fp:
    if parallel and fp.seekable():
      buf_size = 128 << 20  # 128M buffer.
      num_bufs = file_size / buf_size
      logging.debug('num_bufs: %d', num_bufs)
      checkpoint_contents = bytearray(file_size)

      def read_chunk(i):
        # NOTE: We have to re-open the file to read each chunk, otherwise the
        # parallelism has no effect. But we could reuse the file pointers
        # within each thread.
        with open(ckpt_path, 'rb') as f:
          f.seek(i * buf_size)
          buf = f.read(buf_size)
          if buf:
            checkpoint_contents[i * buf_size:i * buf_size + len(buf)] = buf
          return len(buf) / buf_size

      pool_size = 32
      pool = thread.ThreadPoolExecutor(pool_size)
      results = pool.map(read_chunk, range(int(num_bufs) + 1))
      pool.shutdown(wait=False)
      logging.debug(f'results: {list(results)}')
    else:
      checkpoint_contents = fp.read()

  state_dict = msgpack_restore(checkpoint_contents)
  if (get_tensorstore_spec is not None):
    state_dict = _restore_mpas(state_dict, target, ckpt_path, step, gda_manager,
                               allow_partial_mpa_restoration)

  if target is None:
    restored_checkpoint = state_dict
  else:
    restored_checkpoint = from_state_dict(target, state_dict)

  end_time = time.time()
  if monitoring is not None:
    monitoring.record_event_duration_secs(_READ_CHECKPOINT_EVENT, end_time - start_time)

  return restored_checkpoint


def load_pytree(
    filename: str,
    parallel: bool = True,
) -> PyTree:
  """Load the checkpoint from the given checkpoint path.

  Parameters
  ----------
  filename: str
    checkpoint file or directory of checkpoints to restore from.
  parallel: bool
    whether to load seekable checkpoints in parallel, for speed.

  Returns
  -------
  out: Any
    Restored `target` updated from checkpoint file, or if no step specified and
    no checkpoint files present, returns the passed-in `target` unchanged.
    If a file path is specified and is not found, the passed-in `target` will be
    returned. This is to match the behavior of the case where a directory path
    is specified but the directory has not yet been created.
  """
  check_msgpack()
  start_time = time.time()
  if not os.path.exists(filename):
    raise ValueError(f'Checkpoint not found: {filename}')
  sys.stdout.write(f'Loading checkpoint from {filename}\n')
  sys.stdout.flush()
  file_size = os.path.getsize(filename)

  with open(filename, 'rb') as fp:
    if parallel and fp.seekable():
      buf_size = 128 << 20  # 128M buffer.
      num_bufs = file_size / buf_size
      logging.debug('num_bufs: %d', num_bufs)
      checkpoint_contents = bytearray(file_size)

      def read_chunk(i):
        # NOTE: We have to re-open the file to read each chunk, otherwise the
        # parallelism has no effect. But we could reuse the file pointers
        # within each thread.
        with open(filename, 'rb') as f:
          f.seek(i * buf_size)
          buf = f.read(buf_size)
          if buf:
            checkpoint_contents[i * buf_size:i * buf_size + len(buf)] = buf
          return len(buf) / buf_size

      pool_size = 32
      pool = thread.ThreadPoolExecutor(pool_size)
      results = pool.map(read_chunk, range(int(num_bufs) + 1))
      pool.shutdown(wait=False)
      logging.debug(f'results: {list(results)}')
    else:
      checkpoint_contents = fp.read()

  state_dict = msgpack_restore(checkpoint_contents)
  end_time = time.time()
  if monitoring is not None:
    monitoring.record_event_duration_secs(_READ_CHECKPOINT_EVENT, end_time - start_time)

  return state_dict
