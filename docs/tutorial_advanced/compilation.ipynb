{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f48e9b",
   "metadata": {},
   "source": [
    "# JIT Compilation with `BrainPyObject`\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brainpy/brainpy/blob/master/docs/tutorial_advanced/compilation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bb9b6",
   "metadata": {},
   "source": [
    "@[Chaoming Wang](https://github.com/chaoming0625)\n",
    "@[Xiaoyu Chen](mailto:c-xy17@tsinghua.org.cn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a625b0ab",
   "metadata": {},
   "source": [
    "In this section, we are going to talk about code compilation that can accelerate your model running performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e791f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "\n",
    "# bm.set_platform('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd2be7",
   "metadata": {},
   "source": [
    "## ``brainpy.math.jit()``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123027f3",
   "metadata": {},
   "source": [
    "[JAX](https://github.com/google/jax) provides JIT compilation ``jax.jit()`` for [pure functions](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions).In most cases, however, we code with Python classes. ``brainpy.math.jit()`` is intended to extend just-in-time compilation to class objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406eacd",
   "metadata": {},
   "source": [
    "### JIT compilation for class objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae4adb",
   "metadata": {},
   "source": [
    "The constraints for class-object JIT ciompilation include:\n",
    "\n",
    "- The JIT target must be a subclass of ``brainpy.BrainPyObject``.\n",
    "- Dynamically changed variables must be labeled as ``brainpy.math.Variable``.\n",
    "- Updating Variables must be accomplished by in-place operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5374857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(bp.BrainPyObject):\n",
    "    def __init__(self, dimension):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "        # parameters\n",
    "        self.dimension = dimension\n",
    "\n",
    "        # variables\n",
    "        self.w = bm.Variable(2.0 * bm.ones(dimension) - 1.3)\n",
    "\n",
    "    def __call__(self, X, Y):\n",
    "        u = bm.dot(((1.0 / (1.0 + bm.exp(-Y * bm.dot(X, self.w))) - 1.0) * Y), X)\n",
    "        self.w[:] = self.w - u \n",
    "        # The above line can also be expressed as: \n",
    "        # \n",
    "        #   self.w.value = self.w - u \n",
    "        # \n",
    "        # or, \n",
    "        # \n",
    "        #   self.w.update(self.w - u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2440bf0e",
   "metadata": {},
   "source": [
    "In this example, weight *self.w* is a dynamically changed variable, thus marked as ``Variable``. During the update phase ``__call__()``, *self.w* is in-place updated through ``self.w[:] = ...``. Alternatively, one can replace the data in the variable by ``self.w.value = ...`` or ``self.w.update(...)``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893ec359",
   "metadata": {},
   "source": [
    "Now this logistic regression can be accelerated by JIT compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462f745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim, num_points = 10, 200000\n",
    "points = bm.random.random((num_points, num_dim))\n",
    "labels = bm.random.random(num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5e5b98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.11 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "3.99 ms ± 1.85 ms per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(10)\n",
    "\n",
    "%timeit lr(points, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70b1eba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.73 ms ± 129 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "lr_jit = bm.jit(lr)\n",
    "\n",
    "%timeit lr_jit(points, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3b576",
   "metadata": {},
   "source": [
    "### JIT mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1046bd0",
   "metadata": {},
   "source": [
    "The mechanism of JIT compilation is that BrainPy automatically transforms your class methods into functions. \n",
    "\n",
    "``brainpy.math.jit()`` receives a ``dyn_vars`` argument, which denotes the dynamically changed variables. If it is not provided by users, BrainPy will automatically detect them by calling ``BrainPyObject.vars()`` (only variables labeled as `Variable` will be automatically retrieved by BrainPyObject.vars()). Once receiving \"dyn_vars\", BrainPy will treat \"dyn_vars\" as function arguments and then transform class objects into functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42fbe267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JITTransform(target=LogisticRegression0, \n",
       "             num_of_vars=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_jit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f0e22",
   "metadata": {},
   "source": [
    "Therefore, the secrete of ``brainpy.math.jit()`` is providing \"dyn_vars\". No matter your target is a class object, a method in the class object, or a pure function, if there are dynamically changed variables, you just pack them into ``brainpy.math.jit()`` as \"dyn_vars\". Then, all the compilation and acceleration will be handled by BrainPy automatically. Let's illustrate this by several examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d5d84",
   "metadata": {},
   "source": [
    "#### Example 1: JIT compiled methods in a class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e79a7f",
   "metadata": {},
   "source": [
    "In this example, we try to run a method just-in-time in a class, in which the object variable are used to compute the final results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "076fc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(bp.BrainPyObject):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = bm.Variable(bm.random.random((n_in, n_out)))\n",
    "        self.b = bm.Variable(bm.zeros(n_out))\n",
    "    \n",
    "    def update(self, x):\n",
    "        return x @ self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5eca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bm.zeros(10)  # the input data\n",
    "l = Linear(10, 3)  # the class we need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3af6f71",
   "metadata": {},
   "source": [
    "First, we mark \"w\" and \"b\" as dynamically changed variables. Changing \"w\" or \"b\" will change the final results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cca2e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update1 = bm.jit(\n",
    "    l.update, dyn_vars=[l.w, l.b]  # make 'w' and 'b' dynamically change\n",
    ")  \n",
    "\n",
    "update1(x)  # x is 0., b is 0., therefore y is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4c9c2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.b[:] = 1.  # change b to 1, we expect y will be 1 too\n",
    "\n",
    "update1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572c4d8",
   "metadata": {},
   "source": [
    "This time, we only mark \"w\" as a dynamically changed variable. We will find that no matter how \"b\" is modified, the results will not change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7bdf120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update2 = bm.jit(\n",
    "    l.update, dyn_vars=[l.w]  # only make 'w' dynamically change\n",
    ")\n",
    "\n",
    "update2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446ea19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.b[:] = 2.  # change b to 2\n",
    "\n",
    "update2(x)  # while y will not be 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb25ba8",
   "metadata": {},
   "source": [
    "#### Example 2: JIT compiled functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b424f80",
   "metadata": {},
   "source": [
    "Now, we change the above \"Linear\" object to a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "675ce89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in = 10;  n_out = 3\n",
    "\n",
    "w = bm.Variable(bm.random.random((n_in, n_out)))\n",
    "b = bm.Variable(bm.zeros(n_out))\n",
    "\n",
    "def update(x):\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ffb7e",
   "metadata": {},
   "source": [
    "If we do not provide ``dyn_vars``, \"w\" and \"b\" will be compiled as constant values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5e3c1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update1 = bm.jit(update)\n",
    "update1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "922fd101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:] = 1.  # modify the value of 'b' will not \n",
    "           # change the result, because in the \n",
    "           # jitted function, 'b' is already \n",
    "           # a constant\n",
    "update1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbbd220",
   "metadata": {},
   "source": [
    "Providing \"w\" and \"b\" as ``dyn_vars`` will make them dynamically changed again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c301f14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update2 = bm.jit(update, dyn_vars=(w, b))\n",
    "update2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "165bb3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:] = 2.  # change b to 2, while y will not be 2\n",
    "update2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c54d3",
   "metadata": {},
   "source": [
    "#### Example 3: JIT compiled neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a0425",
   "metadata": {},
   "source": [
    "Now, let's use SGD to train a neural network with JIT acceleration. Here we use the autograd function ``brainpy.math.grad()``, which will be discussed in detail in [the next section](./differentiation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b89b7af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class LinearNet(bp.BrainPyObject):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super(LinearNet, self).__init__()\n",
    "\n",
    "        # weights\n",
    "        self.w = bm.TrainVar(bm.random.random((n_in, n_out)))\n",
    "        self.b = bm.TrainVar(bm.zeros(n_out))\n",
    "        self.r = bm.TrainVar(bm.random.random((n_out, 1)))\n",
    "    \n",
    "    def update(self, x):\n",
    "        h = x @ self.w + self.b\n",
    "        return h @ self.r\n",
    "    \n",
    "    def loss(self, x, y):\n",
    "        predict = self.update(x)\n",
    "        return bm.mean((predict - y) ** 2)\n",
    "\n",
    "\n",
    "ln = LinearNet(100, 200)\n",
    "\n",
    "# provide the variables want to update\n",
    "opt = bp.optimizers.SGD(lr=1e-6, train_vars=ln.vars()) \n",
    "\n",
    "# provide the variables require graidents\n",
    "f_grad = bm.grad(ln.loss, grad_vars=ln.vars(), return_value=True)  \n",
    "\n",
    "\n",
    "def train(X, Y):\n",
    "    grads, loss = f_grad(X, Y)\n",
    "    opt.update(grads)\n",
    "    return loss\n",
    "\n",
    "# JIT the train function \n",
    "train_jit = bm.jit(train, dyn_vars=ln.vars() + opt.vars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ae01dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0, loss = 5844618.50\n",
      "Train 1, loss = 3233343.75\n",
      "Train 2, loss = 1798474.62\n",
      "Train 3, loss = 1003369.12\n",
      "Train 4, loss = 560716.69\n",
      "Train 5, loss = 313641.56\n",
      "Train 6, loss = 175531.56\n",
      "Train 7, loss = 98268.23\n",
      "Train 8, loss = 55024.91\n",
      "Train 9, loss = 30816.03\n",
      "Train 10, loss = 17261.26\n",
      "Train 11, loss = 9671.21\n",
      "Train 12, loss = 5420.95\n",
      "Train 13, loss = 3040.83\n",
      "Train 14, loss = 1707.96\n",
      "Train 15, loss = 961.55\n",
      "Train 16, loss = 543.55\n",
      "Train 17, loss = 309.47\n",
      "Train 18, loss = 178.38\n",
      "Train 19, loss = 104.97\n",
      "Train 20, loss = 63.86\n",
      "Train 21, loss = 40.84\n",
      "Train 22, loss = 27.94\n",
      "Train 23, loss = 20.72\n",
      "Train 24, loss = 16.68\n",
      "Train 25, loss = 14.41\n",
      "Train 26, loss = 13.15\n",
      "Train 27, loss = 12.44\n",
      "Train 28, loss = 12.04\n",
      "Train 29, loss = 11.82\n"
     ]
    }
   ],
   "source": [
    "xs = bm.random.random((1000, 100))\n",
    "ys = bm.random.random((1000, 1))\n",
    "\n",
    "for i in range(30):\n",
    "    loss  = train_jit(xs, ys)\n",
    "    print(f'Train {i}, loss = {loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967345db",
   "metadata": {},
   "source": [
    "### RandomState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26106295",
   "metadata": {},
   "source": [
    "We have talked about RandomState in the [Variables](./variables.ipynb) section. RandomeState is also a Variable. Therefore, if the default RandomState (``brainpy.math.random.DEFAULT``) is used in your function, you should mark it as one of the ``dyn_vars`` in the function. Otherwise, they will be treated as constants and the jitted function will always return the same value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe1a5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function():\n",
    "    return bm.random.normal(0, 1, size=(10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93c3d479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True], dtype=bool)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = bm.jit(function)\n",
    "\n",
    "f1() == f1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95276b4b",
   "metadata": {},
   "source": [
    "The correct way to make JIT for this function is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dfba12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([False, False, False, False, False, False, False, False, False,\n",
       "       False], dtype=bool)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm.random.seed(1234)\n",
    "\n",
    "f2 = bm.jit(function, dyn_vars=bm.random.DEFAULT)\n",
    "\n",
    "f2() == f2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f23e60",
   "metadata": {},
   "source": [
    "### Static arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf607131",
   "metadata": {},
   "source": [
    "Static arguments are treated as static/constant in the jitted function. \n",
    "\n",
    "Two things must be marked as static: numerical arguments used in the conditional syntax (bool values or resulting in bool values) and strings. Otherwise, an error will raise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c624ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@bm.jit\n",
    "def f(x):\n",
    "  if x < 3:  # this will cause error\n",
    "    return 3. * x ** 2\n",
    "  else:\n",
    "    return -4 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43d03199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jax._src.errors.ConcretizationTypeError'> Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(bool[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>\n",
      "The problem arose with the `bool` function. \n",
      "The error occurred while tracing the function f at C:\\Users\\adadu\\AppData\\Local\\Temp\\ipykernel_12696\\2472963545.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument 'x'.\n",
      "\n",
      "See https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    f(1.)\n",
    "except Exception as e:\n",
    "    print(type(e), e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa080dcc",
   "metadata": {},
   "source": [
    "Simply speaking, arguments resulting in boolean values must be declared as static arguments. In ``brainpy.math.jit()`` function, we can set the names of static arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3005cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  if x < 3:  # this will cause error\n",
    "    return 3. * x ** 2\n",
    "  else:\n",
    "    return -4 * x\n",
    "\n",
    "f_jit = bm.jit(f, static_argnames=('x',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41349cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(3., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_jit(x=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86485a58",
   "metadata": {},
   "source": [
    "However, it's worth noting that calling the jitted function with different values for these static arguments will trigger recompilation. Therefore, declaring static arguments may be suitable to the following situations:\n",
    "\n",
    "1. Boolean arguments.\n",
    "2. Arguments that only have several possible values. \n",
    "\n",
    "If the argument value change significantly, you'd better not declare it as static. \n",
    "\n",
    "For more information, please refer to the [jax.jit](https://jax.readthedocs.io/en/latest/jax.html?highlight=jit#jax.jit) API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
