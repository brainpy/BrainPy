{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Simulation for Parameter Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "@[Tianqiu Zhang](mailto:tianqiuakita@gmail.com) @[Chaoming Wang](mailto:adaduo@outlook.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter exploration and selection is an essential part in brain dynamics modeling. In general, there are two problems for the parameter exploration:\n",
    "\n",
    "1. how to run multiple models concurrently?\n",
    "2. how to manage device memory allowing multiple models to run concurrently?\n",
    "\n",
    "First, most of the BrainPy models supports multiple kinds of parallelization, including parallelization of multi-threading and multi-processing on a single machine, and parallelization across multiple devices. In the below, we will illustrate these parallelization APIs one-by-one.\n",
    "\n",
    "Second, every call of a BrainPy model will consume a fraction of device memory. Therefore, BrainPy provides a API ``brainpy.math.clear_buffer_memory()`` for memory clean.\n",
    "\n",
    "In the following, we will illustrate how to combine them together to get an efficient parameter exploration for your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainpy.math as bm\n",
    "import numpy as np\n",
    "\n",
    "bm.set_platform('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization across different CPU processors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelization across multiple CPU processors can be easily achieved with a single line of functional call ``brainpy.running.cpu_ordered_parallel()``. The following pseudocode demonstrates the usage of this API."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "import brainpy as bp\n",
    "\n",
    "# define your function\n",
    "def run_model(par):\n",
    "  model = YourModel(par)\n",
    "  runner = bp.dyn.DSRunner(model)\n",
    "  runner.run(duration)\n",
    "  return runner.mon\n",
    "\n",
    "# define all parameter values need to explore\n",
    "all_params = [...]\n",
    "\n",
    "# run models in Jupyter\n",
    "results = bp.running.cpu_ordered_parallel(run_model, all_params, num_process=10)\n",
    "\n",
    "# run models in python file\n",
    "if __name__ == '__main__':\n",
    "  results = bp.running.cpu_ordered_parallel(run_model, all_params, num_process=10)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use a simple HH neuron model as an example to show this kind of parallelization method. In this example, we use multi-processing technique to test four different current values as input."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, define your running function with the well-defined `input` and `output` data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def hh_spike_num(bg_current): # \"input\" is the bg_current\n",
    "  import brainpy as bp  # needed to reimport packages when\n",
    "                        # run the function in Jupyter\n",
    "  model = bp.neurons.HH(1)\n",
    "  runner = bp.dyn.DSRunner(model, monitors=['spike'], inputs=['input', bg_current])\n",
    "  runner.run(1000.)\n",
    "  return runner.mon['spike'].sum()  # \"output\" is the spike number"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, define all your parameter spaces."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "current = bm.linspace(1, 10.1, 10)  # here only one parameter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, run your model concurrently with the  parallelization syntax."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8760d226fe2743619106637872b9b4f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[0, 0, 0, 48, 53, 57, 60, 63, 66, 68]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = bp.running.cpu_ordered_parallel(hh_spike_num, [current], num_process=10)\n",
    "\n",
    "r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, the above usage will accumulate buffer memory in the running device. If your single model occupies too much memory, the out-of-memory error will be raised during the parameter exploration.\n",
    "\n",
    "A simple way to solve this issue is clear all buffers after each running of the function. For example, before returning your results, call `brainpy.math.clear_buffer_memory()` first."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def hh_spike_num2(bg_current): # \"input\" is the bg_current\n",
    "  import brainpy as bp  # needed to reimport packages when\n",
    "                        # run the function in Jupyter\n",
    "\n",
    "  bg_current = bp.math.as_jax(bg_current)\n",
    "  model = bp.neurons.HH(1)\n",
    "  runner = bp.dyn.DSRunner(model, monitors=['spike'], inputs=['input', bg_current])\n",
    "  runner.run(1000.)\n",
    "\n",
    "  bp.math.clear_buffer_memory()\n",
    "  return runner.mon['spike'].sum()  # \"output\" is the spike number"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that ``clear_buffer_memory()`` will clear all JAX arrays in the device, therefore, it's better to give inputs as NumPy arrays, and return outputs as NumPy arrays."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10d02874c8584954a79129168536accd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[0, 0, 1, 47, 0, 57, 60, 63, 66, 68]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current = np.linspace(1., 10., 10)\n",
    "\n",
    "r = bp.running.cpu_ordered_parallel(hh_spike_num2, [current], num_process=10)\n",
    "r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you think that the order of the running results does not matter, you can also use ``cpu_unordered_parallel()`` function. This can maximize the running efficiency of all processors, since all workers run with a non-blocking and unordered manner."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parallelization with `jax.vmap`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second approach of realizing multi-threading parallelization is the vectorization map of JAX `jax.vmap`. `jax.vmap` vectorizes functions by compiling the mapped axis as primitive operations. It can avoid the recompilation of models in the same batch, and automatically parallelize the model running on the given machine. Following pseudocode demonstrates how simple of this parallelization approach is."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "from jax import vmap\n",
    "\n",
    "def run_model(par):\n",
    "  model = YourModel(par)\n",
    "  runner = bp.dyn.DSRunner(model)\n",
    "  runner.run(duration)\n",
    "  return runner.mon\n",
    "\n",
    "# define all parameter values need to explore\n",
    "all_params = [...]\n",
    "\n",
    "# batch simulation through jax.vmap\n",
    "r = vmap(run_model)(*all_params)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that if you have too many parameters to search, ``jax.vmap`` will consume too much memory. For this time, you can use our wrapped API ``brainpy.running.jax_vectorize_map()``, which controls the running batch size by ``num_parallel`` parameter. You can set a smaller value of ``num_parallel`` when your device memory is not enough (no matter on the CPU or GPU device)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def hh_spike_num3(bg_current): # \"input\" is the bg_current\n",
    "  model = bp.neurons.HH(1)\n",
    "  runner = bp.dyn.DSRunner(model, monitors=['spike'], inputs=['input', bg_current],\n",
    "                           numpy_mon_after_run=False)\n",
    "  runner.run(1000.)\n",
    "  return runner.mon['spike'].sum()  # \"output\" is the spike number"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0c1b9e2544c4b7d9324b5a1d257dab8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9e534fb4c06d445b8df7ed5300d25bd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d7cb69912994ee7ac3e44cde95373b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "903708f92cbf4e2086157712080635b7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "JaxArray([ 0,  0,  0,  0,  0,  0, 60, 63, 66, 68], dtype=int32)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current = bm.linspace(1., 10.1, 10)\n",
    "r = bp.running.jax_vectorize_map(hh_spike_num3, [current], num_parallel=3)\n",
    "r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function throw into the ``jax_vectorize_map()`` can not call ``clear_buffer_memory()``. Otherwise will raise errors. Instead, uses can set ``clear_buffer=True/False`` using ``jax_vectorize_map()``. For such kind of usage, all inputs and outputs will be automatically transformed in to NumPy arrays."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff15303b2a394ce488fcf4d65ce3eca8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52b82eeb8c874946aeceaed549fc024b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f974774c2e1948bb87b1307d34bc69ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ba11b94834d435eb1791948c8db2048"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "array([ 0,  0,  0,  0,  0, 55, 58, 59, 65, 68])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current = bm.linspace(1., 10.1, 10)\n",
    "r = bp.running.jax_vectorize_map(hh_spike_num3, [current], num_parallel=3, clear_buffer=True)\n",
    "r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parallelization across multiple devices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "BrainPy support parallelization running on multiple devices (e.g., multiple GPU devices or TPU cores) or HPC systems (e.g., supercomputers). Different from the above thread-based and processor-based parallelization methods, in which the same model runs in parallel on the same device, device-based parallelization runs the same model in parallel on multiple devices.\n",
    "\n",
    "One way to express the multi-device parallelization of BrainPy models is using `jax.pmap` instruction. JAX delivers `jax.pmap` to express SIMD programs. It provides an interface to run the same model on multiple devices with different parameter values. It usage is analogy to `jax.vmap`. Following pseudocode presents an example to run BrainPy models on multiple devices."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "from jax import pmap\n",
    "\n",
    "def run_model(par):\n",
    "  model = YourModel(par)\n",
    "  runner = bp.dyn.DSRunner(model)\n",
    "  runner.run(<int>)\n",
    "  return runner.mon\n",
    "\n",
    "# define all parameter values need to explore\n",
    "all_params = [...]\n",
    "\n",
    "# parallel simulation through jax.pmap\n",
    "r = pmap(run_model)(*all_params)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "``jax.pmap`` has the similar issue to ``jax.vmap`` when you parallelize across many parameters. This time you can use the wrapped function ``brainpy.running.jax_parallelize_map()``.\n",
    "\n",
    "If you are using ``pmap`` in you CPU device, you can set the virtual number of the device by calling ``brainpy.math.set_host_device_count(n)``. Then, you can call ``jax_parallelize_map()`` safely one your CPU platform."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/10000 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffad3289fd574502a4a4862ab1f767fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "array([ 0,  0,  0,  0,  0,  0,  0, 49, 52, 54, 56, 58, 59, 61, 62, 63, 65,\n       66, 67, 68])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bp.math.set_host_device_count(10)  # this should place on the top of the file\n",
    "current = bm.linspace(1., 10.1, 20)\n",
    "r = bp.running.jax_parallelize_map(hh_spike_num3, [current], num_parallel=10, clear_buffer=True)\n",
    "r"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "BrainPy also works well with job scheduling systems such as SLURM on a supercomputer center. Therefore, another way to express multi-device parallelization is to employ the classical resource management system. Following script demonstrates an example that submits a batch script to SLURM."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH -J <name>\n",
    "#SBATCH -o <file name>\n",
    "#SBATCH -p <str>\n",
    "#SBATCH -n <int>\n",
    "#SBATCH -N <int>\n",
    "#SBATCH -c <int>\n",
    "\n",
    "python your_script.py\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "brainpy",
   "language": "python",
   "display_name": "brainpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f37317bd3e2379aba54e3aa76414bc918141342cb86849b10e642bf3607e7693"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
