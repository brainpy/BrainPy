{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Management: The Foundation of ``brainpy.state``\n",
    "\n",
    "State management is the core architectural change in `brainpy.state`. Understanding states is\n",
    "essential for using BrainPy effectively. This guide provides comprehensive coverage of the state\n",
    "system built on `brainstate`.\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "## Overview\n",
    "\n",
    "### What is State?\n",
    "\n",
    "**State** is any variable that persists across function calls and can change over time. In neural simulations:\n",
    "\n",
    "- Membrane potentials\n",
    "- Synaptic conductances\n",
    "- Spike trains\n",
    "- Learnable weights\n",
    "- Temporary buffers\n",
    "\n",
    "**Key insight:** `brainpy.state` makes states **explicit** rather than implicit. Every stateful variable is declared and tracked.\n",
    "\n",
    "### Why Explicit State Management?\n",
    "\n",
    "**Problems with implicit state (BrainPy 2.x):**\n",
    "\n",
    "- Hard to track what changes when\n",
    "- Difficult to serialize/checkpoint\n",
    "- Unclear initialization procedures\n",
    "- Conflicts with JAX functional programming\n",
    "\n",
    "**Benefits of explicit state (`brainpy.state`):**\n",
    "\n",
    "✅ Clear variable lifecycle\n",
    "\n",
    "✅ Easy checkpointing and loading\n",
    "\n",
    "✅ Functional programming compatible\n",
    "\n",
    "✅ Better debugging and introspection\n",
    "\n",
    "✅ Automatic differentiation support\n",
    "\n",
    "✅ Type safety and validation\n",
    "\n",
    "### The State Hierarchy\n",
    "\n",
    "BrainPy uses different state types for different purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "State (base class)\n",
    "│\n",
    "├── ParamState        ← Learnable parameters (weights, biases)\n",
    "├── ShortTermState    ← Temporary dynamics (V, g, spikes)\n",
    "└── LongTermState     ← Persistent but non-learnable (statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each type has different semantics and handling:\n",
    "\n",
    "- **ParamState**: Updated by optimizers, saved in checkpoints\n",
    "- **ShortTermState**: Reset each trial, not saved\n",
    "- **LongTermState**: Saved but not trained\n",
    "\n",
    "## State Types\n",
    "\n",
    "### ParamState: Learnable Parameters\n",
    "\n",
    "**Use for:** Weights, biases, trainable parameters\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- Updated by gradient descent\n",
    "- Saved in model checkpoints\n",
    "- Persistent across trials\n",
    "- Registered with optimizers\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainstate\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class LinearLayer(brainstate.nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Learnable weight matrix\n",
    "        self.W = brainstate.ParamState(\n",
    "            brainstate.random.randn(in_size, out_size) * 0.01\n",
    "        )\n",
    "\n",
    "        # Learnable bias vector\n",
    "        self.b = brainstate.ParamState(\n",
    "            jnp.zeros(out_size)\n",
    "        )\n",
    "\n",
    "    def update(self, x):\n",
    "        # Use parameters in computation\n",
    "        return jnp.dot(x, self.W.value) + self.b.value\n",
    "\n",
    "# Access all parameters\n",
    "layer = LinearLayer(100, 50)\n",
    "params = layer.states(brainstate.ParamState)\n",
    "# Returns: {'W': ParamState(...), 'b': ParamState(...)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common uses:**\n",
    "\n",
    "- Synaptic weights\n",
    "- Neural biases\n",
    "- Time constants (if learning them)\n",
    "- Connectivity matrices (if plastic)\n",
    "\n",
    "### ShortTermState: Temporary Dynamics\n",
    "\n",
    "**Use for:** Variables that reset each trial\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- Reset at trial start\n",
    "- Not saved in checkpoints\n",
    "- Represent current dynamics\n",
    "- Fastest state type\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainpy as bp\n",
    "import brainunit as u\n",
    "\n",
    "class LIFNeuron(brainstate.nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = size\n",
    "        self.V_rest = -65.0 * u.mV\n",
    "        self.V_th = -50.0 * u.mV\n",
    "\n",
    "        # Membrane potential (resets each trial)\n",
    "        self.V = brainstate.ShortTermState(\n",
    "            jnp.ones(size) * self.V_rest.to_decimal(u.mV)\n",
    "        )\n",
    "\n",
    "        # Spike indicator (resets each trial)\n",
    "        self.spike = brainstate.ShortTermState(\n",
    "            jnp.zeros(size)\n",
    "        )\n",
    "\n",
    "    def reset_state(self, batch_size=None):\n",
    "        \"\"\"Called at trial start.\"\"\"\n",
    "        if batch_size is None:\n",
    "            self.V.value = jnp.ones(self.size) * self.V_rest.to_decimal(u.mV)\n",
    "            self.spike.value = jnp.zeros(self.size)\n",
    "        else:\n",
    "            self.V.value = jnp.ones((batch_size, self.size)) * self.V_rest.to_decimal(u.mV)\n",
    "            self.spike.value = jnp.zeros((batch_size, self.size))\n",
    "\n",
    "    def update(self, I):\n",
    "        # Update membrane potential (simplified example)\n",
    "        new_V = self.V.value + I.to_decimal(u.mV) * 0.1\n",
    "        new_spike = (new_V >= self.V_th.to_decimal(u.mV)).astype(float)\n",
    "        \n",
    "        self.V.value = new_V\n",
    "        self.spike.value = new_spike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common uses:**\n",
    "\n",
    "- Membrane potentials\n",
    "- Synaptic conductances\n",
    "- Spike indicators\n",
    "- Refractory counters\n",
    "- Temporary buffers\n",
    "\n",
    "### LongTermState: Persistent Non-Learnable\n",
    "\n",
    "**Use for:** Statistics, counters, persistent metadata\n",
    "\n",
    "**Characteristics:**\n",
    "\n",
    "- Not reset each trial\n",
    "- Saved in checkpoints\n",
    "- Not updated by optimizers\n",
    "- Accumulates over time\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronWithStatistics(brainstate.nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.V = brainstate.ShortTermState(jnp.zeros(size))\n",
    "        self.spike = brainstate.ShortTermState(jnp.zeros(size))\n",
    "\n",
    "        # Running spike count (persists across trials)\n",
    "        self.total_spikes = brainstate.LongTermState(\n",
    "            jnp.zeros(size, dtype=jnp.int32)\n",
    "        )\n",
    "\n",
    "        # Running average firing rate\n",
    "        self.avg_rate = brainstate.LongTermState(\n",
    "            jnp.zeros(size)\n",
    "        )\n",
    "\n",
    "    def update(self, I):\n",
    "        # ... update dynamics ...\n",
    "        # (Simplified example)\n",
    "        self.spike.value = (self.V.value > 0).astype(float)\n",
    "\n",
    "        # Accumulate statistics\n",
    "        self.total_spikes.value += self.spike.value.astype(jnp.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common uses:**\n",
    "\n",
    "- Spike counters\n",
    "- Running averages\n",
    "- Homeostatic variables\n",
    "- Simulation metadata\n",
    "- Custom statistics\n",
    "\n",
    "## State Initialization\n",
    "\n",
    "### Automatic Initialization\n",
    "\n",
    "BrainPy provides `init_all_states()` for automatic initialization.\n",
    "\n",
    "**Basic usage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyNetwork(\n",
       "  neuron=LIF(\n",
       "    in_size=(100,),\n",
       "    out_size=(100,),\n",
       "    spk_reset=soft,\n",
       "    spk_fun=ReluGrad(alpha=0.3, width=1.0),\n",
       "    R=1. * ohm,\n",
       "    tau=10 * msecond,\n",
       "    V_th=-50 * mvolt,\n",
       "    V_rest=-65 * mvolt,\n",
       "    V_reset=0. * mvolt,\n",
       "    V_initializer=Constant(value=0.0 * mvolt),\n",
       "    V=HiddenState(\n",
       "      value=~float32[32,100] * mvolt\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import brainstate\n",
    "import brainpy as bp\n",
    "import brainunit as u\n",
    "\n",
    "# Define a simple network for demonstration\n",
    "class MyNetwork(brainstate.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.neuron = bp.state.LIF(100, V_rest=-65*u.mV, V_th=-50*u.mV, tau=10*u.ms)\n",
    "    \n",
    "    def update(self, inp):\n",
    "        return self.neuron(inp)\n",
    "\n",
    "# Create network\n",
    "net = MyNetwork()\n",
    "\n",
    "# Initialize all states (single trial)\n",
    "brainstate.nn.init_all_states(net)\n",
    "\n",
    "# Initialize with batch dimension\n",
    "brainstate.nn.init_all_states(net, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What it does:**\n",
    "\n",
    "1. Finds all modules in the hierarchy\n",
    "2. Calls `reset_state()` on each module\n",
    "3. Handles nested structures automatically\n",
    "4. Sets up batch dimensions if requested\n",
    "\n",
    "**Example with network:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EINetwork(\n",
       "  E=LIF(\n",
       "    in_size=(800,),\n",
       "    out_size=(800,),\n",
       "    spk_reset=soft,\n",
       "    spk_fun=ReluGrad(alpha=0.3, width=1.0),\n",
       "    R=1. * ohm,\n",
       "    tau=10 * msecond,\n",
       "    V_th=-50 * mvolt,\n",
       "    V_rest=-65 * mvolt,\n",
       "    V_reset=0. * mvolt,\n",
       "    V_initializer=Constant(value=0.0 * mvolt),\n",
       "    V=HiddenState(\n",
       "      value=~float32[10,800] * mvolt\n",
       "    )\n",
       "  ),\n",
       "  I=LIF(\n",
       "    in_size=(200,),\n",
       "    out_size=(200,),\n",
       "    spk_reset=soft,\n",
       "    spk_fun=ReluGrad(alpha=0.3, width=1.0),\n",
       "    R=1. * ohm,\n",
       "    tau=10 * msecond,\n",
       "    V_th=-50 * mvolt,\n",
       "    V_rest=-65 * mvolt,\n",
       "    V_reset=0. * mvolt,\n",
       "    V_initializer=Constant(value=0.0 * mvolt),\n",
       "    V=HiddenState(\n",
       "      value=~float32[10,200] * mvolt\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EINetwork(brainstate.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.E = bp.state.LIF(800, V_rest=-65*u.mV, V_th=-50*u.mV, tau=10*u.ms)\n",
    "        self.I = bp.state.LIF(200, V_rest=-65*u.mV, V_th=-50*u.mV, tau=10*u.ms)\n",
    "        # ... projections ...\n",
    "\n",
    "net = EINetwork()\n",
    "\n",
    "# This initializes E, I, and all projections\n",
    "brainstate.nn.init_all_states(net, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Initialization\n",
    "\n",
    "For custom initialization, override `reset_state()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNeuron(brainstate.nn.Module):\n",
    "    def __init__(self, size, V_init_range=(-70, -60)):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.V_init_range = V_init_range\n",
    "\n",
    "        self.V = brainstate.ShortTermState(jnp.zeros(size))\n",
    "\n",
    "    def reset_state(self, batch_size=None):\n",
    "        \"\"\"Custom initialization: random voltage in range.\"\"\"\n",
    "\n",
    "        # Generate random initial voltages\n",
    "        low, high = self.V_init_range\n",
    "        if batch_size is None:\n",
    "            init_V = brainstate.random.uniform(low, high, size=self.size)\n",
    "        else:\n",
    "            init_V = brainstate.random.uniform(low, high, size=(batch_size, self.size))\n",
    "\n",
    "        self.V.value = init_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best practices:**\n",
    "\n",
    "- Always check `batch_size` parameter\n",
    "- Handle both single and batched cases\n",
    "- Initialize all ShortTermStates\n",
    "- Don't initialize ParamStates (they're learnable)\n",
    "- Don't initialize LongTermStates (they persist)\n",
    "\n",
    "### Initializers for Parameters\n",
    "\n",
    "Use `braintools.init` for parameter initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braintools.init as init\n",
    "\n",
    "class Network(brainstate.nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Xavier/Glorot initialization\n",
    "        self.W1 = brainstate.ParamState(\n",
    "            init.XavierNormal()(shape=(in_size, 100))\n",
    "        )\n",
    "\n",
    "        # Kaiming/He initialization (for ReLU)\n",
    "        self.W2 = brainstate.ParamState(\n",
    "            init.KaimingNormal()(shape=(100, out_size))\n",
    "        )\n",
    "\n",
    "        # Zero initialization\n",
    "        self.b = brainstate.ParamState(\n",
    "            init.Constant(0.0)(shape=(out_size,))\n",
    "        )\n",
    "\n",
    "        # Orthogonal initialization (for RNNs)\n",
    "        self.W_rec = brainstate.ParamState(\n",
    "            init.Orthogonal()(shape=(100, 100))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Available initializers:**\n",
    "\n",
    "- `Constant(value)` - Fill with constant\n",
    "- `Normal(mean, std)` - Gaussian distribution\n",
    "- `Uniform(low, high)` - Uniform distribution\n",
    "- `XavierNormal()` - Xavier/Glorot normal\n",
    "- `XavierUniform()` - Xavier/Glorot uniform\n",
    "- `KaimingNormal()` - He normal (for ReLU)\n",
    "- `KaimingUniform()` - He uniform\n",
    "- `Orthogonal()` - Orthogonal matrix (for RNNs)\n",
    "- `Identity()` - Identity matrix\n",
    "\n",
    "## State Access and Manipulation\n",
    "\n",
    "### Reading State Values\n",
    "\n",
    "Access the current value with `.value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "neuron = bp.state.LIF(100, V_rest=-65*u.mV, V_th=-50*u.mV, tau=10*u.ms)\n",
    "brainstate.nn.init_all_states(neuron)\n",
    "\n",
    "# Read current membrane potential\n",
    "current_V = neuron.V.value\n",
    "\n",
    "# Read shape\n",
    "print(current_V.shape)  # (100,)\n",
    "\n",
    "# Read specific neurons\n",
    "V_neuron_0 = neuron.V.value[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing State Values\n",
    "\n",
    "Update state by assigning to `.value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set new value (entire array)\n",
    "neuron.V.value = jnp.ones(100) * -60.0\n",
    "\n",
    "# Update subset\n",
    "neuron.V.value = neuron.V.value.at[0:10].set(-55.0)\n",
    "\n",
    "# Increment\n",
    "neuron.V.value = neuron.V.value + 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Always assign to `.value`, not the state object itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example values\n",
    "new_V = jnp.ones(100) * -60.0\n",
    "\n",
    "# CORRECT\n",
    "neuron.V.value = new_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting States\n",
    "\n",
    "Get all states of a specific type from a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Define a simple network\n",
    "example_net = MyNetwork()\n",
    "\n",
    "# Get all parameters\n",
    "params = example_net.states(brainstate.ParamState)\n",
    "# Returns: dict with parameter names as keys\n",
    "\n",
    "# Get all short-term states\n",
    "short_term = example_net.states(brainstate.ShortTermState)\n",
    "\n",
    "# Get all states (any type)\n",
    "all_states = example_net.states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(brainstate.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = brainstate.ParamState(jnp.ones((10, 10)))\n",
    "        self.V = brainstate.ShortTermState(jnp.zeros(10))\n",
    "\n",
    "net = SimpleNet()\n",
    "\n",
    "params = net.states(brainstate.ParamState)\n",
    "# {'W': ParamState(...)}\n",
    "\n",
    "states = net.states(brainstate.ShortTermState)\n",
    "# {'V': ShortTermState(...)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State in Training\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "Use `brainstate.transform.grad()` to compute gradients w.r.t. parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data for demonstration\n",
    "X = jnp.ones((10, 10))  # 10 samples, 10 features\n",
    "y = jnp.ones((10, 10))  # 10 targets\n",
    "\n",
    "def loss_fn(params, net, X, y):\n",
    "    \"\"\"Loss function parameterized by params.\"\"\"\n",
    "    # params is automatically used by net\n",
    "    output = net(X)\n",
    "    return jnp.mean((output - y) ** 2)\n",
    "\n",
    "# Get parameters\n",
    "params = net.states(brainstate.ParamState)\n",
    "\n",
    "# Compute gradients (if parameters exist)\n",
    "if len(params) > 0:\n",
    "    grads = brainstate.transform.grad(loss_fn, params)(net, X, y)\n",
    "    # grads has same structure as params\n",
    "    # grads = {'W': gradient_for_W, 'b': gradient_for_b, ...}\n",
    "else:\n",
    "    print(\"No trainable parameters in this network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key points:**\n",
    "\n",
    "- Gradients computed only for ParamState\n",
    "- ShortTermState treated as constants\n",
    "- Gradient structure matches parameter structure\n",
    "\n",
    "### Optimizer Updates\n",
    "\n",
    "Register parameters with optimizer and update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braintools\n",
    "\n",
    "# Create optimizer (use 'lr' not 'learning_rate')\n",
    "optimizer = braintools.optim.Adam(lr=1e-3)\n",
    "\n",
    "# Register trainable parameters\n",
    "params = net.states(brainstate.ParamState)\n",
    "if len(params) > 0:\n",
    "    optimizer.register_trainable_weights(params)\n",
    "\n",
    "# Training loop (example structure)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        X, y = batch\n",
    "\n",
    "        # Compute gradients\n",
    "        grads = brainstate.transform.grad(\n",
    "            loss_fn,\n",
    "            params,\n",
    "            return_value=False\n",
    "        )(net, X, y)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The optimizer automatically:**\n",
    "\n",
    "- Updates all registered parameters\n",
    "- Applies learning rate\n",
    "- Handles momentum/adaptive rates\n",
    "- Maintains optimizer state (momentum buffers, etc.)\n",
    "\n",
    "### State Persistence\n",
    "\n",
    "Training doesn't reset ShortTermState between batches (unless you do it manually)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Training with state reset each example\n",
    "# (Pseudocode - demonstrates the pattern)\n",
    "\n",
    "import braintools\n",
    "\n",
    "# Prepare dummy data\n",
    "data_loader = [(jnp.ones(100) * u.nA, jnp.zeros(100)) for _ in range(5)]\n",
    "\n",
    "# Training loop with state reset each example\n",
    "for X, y in data_loader:\n",
    "    # Reset dynamics for new example\n",
    "    brainstate.nn.init_all_states(net)\n",
    "\n",
    "    # Forward pass (dynamics evolve)\n",
    "    output = net(X)\n",
    "\n",
    "    # Backward pass\n",
    "    params = net.states(brainstate.ParamState)\n",
    "    if len(params) > 0:\n",
    "        grads = brainstate.transform.grad(loss_fn, params)(net, X, y)\n",
    "        optimizer.update(grads)\n",
    "\n",
    "# Training with persistent state (e.g., RNN)\n",
    "for X, y in data_loader:\n",
    "    # Don't reset - state carries over\n",
    "    output = net(X)\n",
    "    params = net.states(brainstate.ParamState)\n",
    "    if len(params) > 0:\n",
    "        grads = brainstate.transform.grad(loss_fn, params)(net, X, y)\n",
    "        optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "### Batch Dimensions\n",
    "\n",
    "States can have a batch dimension for parallel trials.\n",
    "\n",
    "**Single trial:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIF(\n",
       "  in_size=(100,),\n",
       "  out_size=(100,),\n",
       "  spk_reset=soft,\n",
       "  spk_fun=ReluGrad(alpha=0.3, width=1.0),\n",
       "  R=1. * ohm,\n",
       "  tau=10 * msecond,\n",
       "  V_th=-50 * mvolt,\n",
       "  V_rest=-65 * mvolt,\n",
       "  V_reset=0. * mvolt,\n",
       "  V_initializer=Constant(value=0.0 * mvolt),\n",
       "  V=HiddenState(\n",
       "    value=~float32[100] * mvolt\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = bp.state.LIF(100, V_rest=-65*u.mV, V_th=-50*u.mV, tau=10*u.ms)\n",
    "brainstate.nn.init_all_states(neuron)\n",
    "# neuron.V.value.shape = (100,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batched trials:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIF(\n",
       "  in_size=(100,),\n",
       "  out_size=(100,),\n",
       "  spk_reset=soft,\n",
       "  spk_fun=ReluGrad(alpha=0.3, width=1.0),\n",
       "  R=1. * ohm,\n",
       "  tau=10 * msecond,\n",
       "  V_th=-50 * mvolt,\n",
       "  V_rest=-65 * mvolt,\n",
       "  V_reset=0. * mvolt,\n",
       "  V_initializer=Constant(value=0.0 * mvolt),\n",
       "  V=HiddenState(\n",
       "    value=~float32[32,100] * mvolt\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron = bp.state.LIF(100, V_rest=-65*u.mV, V_th=-50*u.mV, tau=10*u.ms)\n",
    "brainstate.nn.init_all_states(neuron, batch_size=32)\n",
    "# neuron.V.value.shape = (32, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usage:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input also needs batch dimension\n",
    "inp = brainstate.random.rand(32, 100) * 2.0 * u.nA\n",
    "\n",
    "# Update operates on all batches in parallel\n",
    "neuron(inp)\n",
    "\n",
    "# Output has batch dimension\n",
    "spikes = neuron.get_spike()  # shape: (32, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Batching\n",
    "\n",
    "**1. Parallelism:** GPU processes all batches simultaneously\n",
    "\n",
    "**2. Statistical averaging:** Reduce noise in gradients\n",
    "\n",
    "**3. Exploration:** Try different initial conditions\n",
    "\n",
    "**4. Efficiency:** Amortize compilation cost\n",
    "\n",
    "**Example: Parameter sweep with batching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 10 different input currents in parallel\n",
    "batch_size = 10\n",
    "neuron_batched = bp.state.LIF(100, V_rest=-65*u.mV, V_th=-50*u.mV, tau=10*u.ms)\n",
    "brainstate.nn.init_all_states(neuron_batched, batch_size=batch_size)\n",
    "\n",
    "# Different input for each batch\n",
    "currents = jnp.linspace(0, 5, batch_size).reshape(-1, 1) * u.nA\n",
    "inp_batched = jnp.broadcast_to(currents, (batch_size, 100))\n",
    "\n",
    "# Simulate (example - shortened for demonstration)\n",
    "# for _ in range(1000):\n",
    "#     neuron_batched(inp_batched)\n",
    "\n",
    "# Analyze each trial separately\n",
    "# spike_counts = jnp.sum(neuron_batched.spike.value, axis=1)  # (10,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing and Serialization\n",
    "\n",
    "### Saving Models\n",
    "\n",
    "Save model state to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Example: Saving checkpoint (pseudocode)\n",
    "current_epoch = 10  # Example epoch number\n",
    "# \n",
    "# # Get all states to save\n",
    "state_dict = {\n",
    "    'params': net.states(brainstate.ParamState),\n",
    "    'long_term': net.states(brainstate.LongTermState),\n",
    "    'epoch': current_epoch,\n",
    "    'optimizer_state': optimizer.state_dict()  # If applicable\n",
    "}\n",
    "# \n",
    "# Save to file\n",
    "with open('checkpoint.pkl', 'wb') as f:\n",
    "    pickle.dump(state_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Don't save ShortTermState (it resets each trial).\n",
    "\n",
    "### Loading Models\n",
    "\n",
    "Restore model state from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Loading checkpoint\n",
    "with open('checkpoint.pkl', 'rb') as f:\n",
    "    state_dict = pickle.load(f)\n",
    "\n",
    "# Create fresh model\n",
    "net = MyNetwork()\n",
    "brainstate.nn.init_all_states(net)\n",
    "\n",
    "# Restore parameters\n",
    "params = state_dict['params']\n",
    "for name, param_state in params.items():\n",
    "    # Find corresponding parameter in net and copy value\n",
    "    net_params = net.states(brainstate.ParamState)\n",
    "    if name in net_params:\n",
    "        net_params[name].value = param_state.value\n",
    "\n",
    "# Restore long-term states similarly\n",
    "\n",
    "# Restore optimizer if continuing training\n",
    "optimizer.load_state_dict(state_dict['optimizer_state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Checkpointing\n",
    "\n",
    "**1. Save regularly during training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "save_interval = 5\n",
    "\n",
    "if epoch % save_interval == 0:\n",
    "    save_checkpoint(net, optimizer, epoch, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Keep multiple checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "save_path = f'checkpoint_epoch_{epoch}.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Save best model separately**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = 0.5\n",
    "best_val_loss = 1.0\n",
    "\n",
    "if val_loss < best_val_loss:\n",
    "    best_val_loss = val_loss\n",
    "    save_checkpoint(net, optimizer, epoch, 'best_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Include metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "state_dict = {\n",
    "    'params': net.states(brainstate.ParamState),\n",
    "    'epoch': 10,\n",
    "    'best_val_loss': 0.5,\n",
    "    'config': {'lr': 1e-3, 'batch_size': 32},  # Hyperparameters\n",
    "    'timestamp': datetime.now()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Patterns\n",
    "\n",
    "### Pattern 1: Resetting Between Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simulate multiple trials (pseudocode)\n",
    "num_trials = 10\n",
    "trial_length = 100\n",
    "\n",
    "def get_input(trial, t):\n",
    "    \"\"\"Generate input for given trial and time.\"\"\"\n",
    "    return jnp.ones(100) * 5.0 * u.nA\n",
    "\n",
    "def record(output):\n",
    "    \"\"\"Record output for analysis.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Simulate multiple trials\n",
    "for trial in range(num_trials):\n",
    "    # Reset dynamics\n",
    "    brainstate.nn.init_all_states(net)\n",
    "\n",
    "    # Run trial\n",
    "    for t in range(trial_length):\n",
    "        inp = get_input(trial, t)\n",
    "        output = net(inp)\n",
    "        record(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Accumulating Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronWithStats(brainstate.nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.V = brainstate.ShortTermState(jnp.zeros(size))\n",
    "\n",
    "        # Accumulate across trials\n",
    "        self.total_spikes = brainstate.LongTermState(\n",
    "            jnp.zeros(size, dtype=jnp.int32)\n",
    "        )\n",
    "        self.n_steps = brainstate.LongTermState(0)\n",
    "\n",
    "    def update(self, I):\n",
    "        # ... dynamics ...\n",
    "\n",
    "        # Accumulate\n",
    "        self.total_spikes.value += self.spike.value.astype(jnp.int32)\n",
    "        self.n_steps.value += 1\n",
    "\n",
    "    def get_firing_rate(self):\n",
    "        \"\"\"Average firing rate across all trials.\"\"\"\n",
    "        dt = brainstate.environ.get_dt()\n",
    "        total_time = self.n_steps.value * dt.to_decimal(u.second)\n",
    "        return self.total_spikes.value / total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Conditional Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveNeuron(brainstate.nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.V = brainstate.ShortTermState(jnp.zeros(size))\n",
    "        self.spike = brainstate.ShortTermState(jnp.zeros(size))\n",
    "        self.threshold = brainstate.ParamState(jnp.ones(size) * (-50.0))\n",
    "\n",
    "    def update(self, I):\n",
    "        # Dynamics (simplified)\n",
    "        self.spike.value = (self.V.value > self.threshold.value).astype(float)\n",
    "\n",
    "        # Homeostatic threshold adaptation\n",
    "        # Simplified spike rate computation\n",
    "        spike_rate = jnp.mean(self.spike.value) * 1000.0  # Assume dt=1ms\n",
    "\n",
    "        # Adjust threshold based on activity\n",
    "        target_rate = 5.0  # Hz\n",
    "        adjustment = 0.01 * (spike_rate - target_rate)\n",
    "\n",
    "        # Update learnable threshold\n",
    "        self.threshold.value -= adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 4: Hierarchical States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(brainstate.nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.W = brainstate.ParamState(jnp.ones((in_size, out_size)) * 0.01)\n",
    "    \n",
    "    def update(self, x):\n",
    "        return jnp.dot(x, self.W.value)\n",
    "\n",
    "class HierarchicalNetwork(brainstate.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Submodules have their own states\n",
    "        self.layer1 = MyLayer(100, 50)\n",
    "        self.layer2 = MyLayer(50, 10)\n",
    "\n",
    "    def update(self, x):\n",
    "        # Each layer manages its own states\n",
    "        h1 = self.layer1(x)\n",
    "        h2 = self.layer2(h1)\n",
    "        return h2\n",
    "\n",
    "net = HierarchicalNetwork()\n",
    "\n",
    "# Collect ALL states from hierarchy\n",
    "all_params = net.states(brainstate.ParamState)\n",
    "# Includes params from layer1 AND layer2\n",
    "\n",
    "# Initialize ALL states in hierarchy\n",
    "brainstate.nn.init_all_states(net)\n",
    "# Calls reset_state() on net, layer1, and layer2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topics\n",
    "\n",
    "### Custom State Types\n",
    "\n",
    "Create custom state types for specialized needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomState(brainstate.State):\n",
    "    \"\"\"State that re-randomizes on reset.\"\"\"\n",
    "\n",
    "    def __init__(self, shape, low=0.0, high=1.0):\n",
    "        super().__init__(jnp.zeros(shape))\n",
    "        self.shape = shape\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Re-randomize on reset.\"\"\"\n",
    "        self.value = brainstate.random.uniform(\n",
    "            self.low, self.high, size=self.shape\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Sharing\n",
    "\n",
    "Share state between modules (use with caution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleA(brainstate.nn.Module):\n",
    "    def __init__(self, shared_W):\n",
    "        super().__init__()\n",
    "        self.W = shared_W\n",
    "\n",
    "class ModuleB(brainstate.nn.Module):\n",
    "    def __init__(self, shared_W):\n",
    "        super().__init__()\n",
    "        self.W = shared_W\n",
    "\n",
    "class SharedState(brainstate.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Shared weight matrix\n",
    "        shared_W = brainstate.ParamState(jnp.ones((100, 100)))\n",
    "        self.module1 = ModuleA(shared_W)\n",
    "        self.module2 = ModuleB(shared_W)\n",
    "    # module1 and module2 both modify the same weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use:** Siamese networks, weight tying, parameter sharing\n",
    "\n",
    "**Caution:** Makes dependencies implicit, harder to debug\n",
    "\n",
    "### State Inspection\n",
    "\n",
    "Debug by inspecting state values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all parameter shapes\n",
    "params = net.states(brainstate.ParamState)\n",
    "for name, state in params.items():\n",
    "    print(f\"{name}: {state.value.shape}\")\n",
    "\n",
    "# Check for NaN values\n",
    "for name, state in params.items():\n",
    "    if jnp.any(jnp.isnan(state.value)):\n",
    "        print(f\"NaN detected in {name}!\")\n",
    "\n",
    "# Compute statistics\n",
    "V_values = neuron.V.value\n",
    "print(f\"V range: [{V_values.min():.2f}, {V_values.max():.2f}]\")\n",
    "print(f\"V mean: {V_values.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Issue: States not updating\n",
    "\n",
    "**Symptoms:** Values stay constant\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. Assign to `.value`, not the state itself\n",
    "2. Check you're updating the right variable\n",
    "3. Verify update function is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of correct vs wrong state assignment\n",
    "example_V = jnp.ones(100) * -60.0\n",
    "\n",
    "# WRONG - Creates new object, doesn't update state!\n",
    "self.V = example_V\n",
    "\n",
    "# CORRECT - Updates state value\n",
    "self.V.value = example_V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue: Batch dimension errors\n",
    "\n",
    "**Symptoms:** Shape mismatch errors\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. Initialize with `batch_size` parameter\n",
    "2. Ensure inputs have batch dimension\n",
    "3. Check `reset_state()` handles batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with batching\n",
    "brainstate.nn.init_all_states(net, batch_size=32)\n",
    "\n",
    "# Input needs batch dimension\n",
    "inp = jnp.zeros((32, 100))  # (batch, neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue: Gradients are None\n",
    "\n",
    "**Symptoms:** No gradients for parameters\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. Ensure parameters are `ParamState`\n",
    "2. Check parameters are used in loss computation\n",
    "3. Verify gradient function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Ensure parameters are ParamState\n",
    "init_W = jnp.ones((100, 50)) * 0.01\n",
    "\n",
    "# Parameters must be ParamState\n",
    "self.W = brainstate.ParamState(init_W)  # Correct\n",
    "\n",
    "# Compute gradients for parameters only\n",
    "params = net.states(brainstate.ParamState)\n",
    "grads = brainstate.transform.grad(loss_fn, params)(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue: Memory leak during training\n",
    "\n",
    "**Symptoms:** Memory grows over time\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. Don't accumulate history in Python lists\n",
    "2. Clear unnecessary references\n",
    "3. Use `jnp.array` operations (not Python append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Avoid memory leaks\n",
    "inp = jnp.ones(100) * 5.0 * u.nA\n",
    "\n",
    "# BAD - accumulates in Python memory\n",
    "history = []\n",
    "for t in range(10000):\n",
    "    output = net(inp)\n",
    "    history.append(output)  # Memory leak!\n",
    "\n",
    "# GOOD - use fixed-size buffer or don't store\n",
    "for t in range(10000):\n",
    "    output = net(inp)\n",
    "#     # Process immediately, don't store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- architecture - Overall BrainPy architecture\n",
    "- neurons - Neuron models and their states\n",
    "- synapses - Synapse models and their states\n",
    "- ../tutorials/advanced/05-snn-training - Training with states\n",
    "- BrainState documentation: https://brainstate.readthedocs.io/\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "✅ **Three state types:**\n",
    "   - `ParamState`: Learnable parameters\n",
    "   - `ShortTermState`: Temporary dynamics\n",
    "   - `LongTermState`: Persistent statistics\n",
    "\n",
    "✅ **Initialization:**\n",
    "   - Use `brainstate.nn.init_all_states(module)`\n",
    "   - Implement `reset_state()` for custom logic\n",
    "   - Handle batch dimensions\n",
    "\n",
    "✅ **Access:**\n",
    "   - Read/write with `.value`\n",
    "   - Collect with `.states(StateType)`\n",
    "   - Never assign to state object directly\n",
    "\n",
    "✅ **Training:**\n",
    "   - Gradients computed for `ParamState`\n",
    "   - Register with optimizer\n",
    "   - Update with `optimizer.update(grads)`\n",
    "\n",
    "✅ **Checkpointing:**\n",
    "   - Save `ParamState` and `LongTermState`\n",
    "   - Don't save `ShortTermState`\n",
    "   - Include metadata and optimizer state\n",
    "\n",
    "**Quick reference:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference example (pseudocode)\n",
    "\n",
    "# Define example initializers\n",
    "init_W = jnp.ones((100, 50)) * 0.01\n",
    "init_V = jnp.zeros(100)\n",
    "init_c = jnp.zeros(100, dtype=jnp.int32)\n",
    "\n",
    "# Define states\n",
    "class MyModule(brainstate.nn.Module):\n",
    "    def __init__(self, size=100):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.W = brainstate.ParamState(init_W)           # Learnable\n",
    "        self.V = brainstate.ShortTermState(init_V)       # Resets\n",
    "        self.count = brainstate.LongTermState(init_c)    # Persists\n",
    "\n",
    "    def reset_state(self, batch_size=None):\n",
    "        \"\"\"Initialize ShortTermState.\"\"\"\n",
    "        shape = self.size if batch_size is None else (batch_size, self.size)\n",
    "        self.V.value = jnp.zeros(shape)\n",
    "\n",
    "# Initialize\n",
    "module = MyModule()\n",
    "brainstate.nn.init_all_states(module, batch_size=32)\n",
    "\n",
    "# Access\n",
    "params = module.states(brainstate.ParamState)\n",
    "new_V = jnp.ones(100) * -60.0\n",
    "module.V.value = new_V\n",
    "\n",
    "# Train\n",
    "def loss(params, module, X, y):\n",
    "    return jnp.mean((module.update(X) - y) ** 2)\n",
    "grads = brainstate.transform.grad(loss, params)(module, X, y)\n",
    "# optimizer.update(grads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ecosystem-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
